% Regarding `oneside` (https://stackoverflow.com/a/8371473/630364):
%
% `oneside` removes the blank pages between chapters.
% "Note that this method make the margins of all the pages the same. In
% `twoside`, the margins are different for the odd and the even pages".
\documentclass[12pt, letterpaper, oneside]{book}
\usepackage{pseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{float}
\usepackage{hyperref}
\usepackage[letterpaper, textwidth=7.5in, textheight=8in]{geometry}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}
\usepackage{titlesec}
\usepackage{parskip}
\usepackage{xcolor}

\setcounter{secnumdepth}{4}

\title{
  Notes on \textit{Basic Analysis I: Introduction to Real Analysis, Volume 1}
}
\author{Yaobin Wen}
\date{July 2023}

\begin{document}

\maketitle
\tableofcontents

\chapter*{Overview}
\addcontentsline{toc}{chapter}{Overview}

This document contains my study notes of the textbook \textit{Basic Analysis I:
  Introduction to Real Analysis, Volume 1}. I use it for a few purposes:

\begin{enumerate}
  \item As a reference to quickly refresh my memory on the subjects.
  \item Keep the notes to help me understand the text that is not obvious for
        me to comprehend.
\end{enumerate}

% =============================================================================
%
% Chapter: Review of Real Numbers
%
% =============================================================================

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

% =============================================================================
\section{About this book}
% =============================================================================

Nothing to write down.

% =============================================================================
\section{About analysis}
% =============================================================================

The \textbf{most important difference} between analysis and algebra:
\begin{itemize}
  \item In algebra, we prove \textbf{equalities} directly.
  \item In analysis, we usually prove \textbf{inequalities} and we prove those
        inequalities by estimating.
\end{itemize}

% =============================================================================
\section{Basic set theory}
% =============================================================================

% ******************************
\subsection{Sets}
% ******************************

\textbf{Definition 0.3.1.} A \textit{set} is a collection of objects called
\textit{elements} or \textit{members}. A set with no objects is called the
\textit{empty set} and is denoted by $\emptyset$ (or sometimes by $\{\}$).

The \textbf{universe} is the set that contains only the elements that we are
interested in. It is generally understood from context and is not explicitly
mentioned.

The various set-related notations:
\begin{itemize}
  \item \textbf{Membership}:
        \begin{itemize}
          \item[$\bullet$] $a \in A$: the element $a$ belongs to the set $A$
          \item[$\bullet$] $a \notin A$: the element $a$ does not belong to the set
                $A$.
        \end{itemize}
  \item \textbf{Subset}:
        \begin{itemize}
          \item Used in this book: $A \subset B$ (at times: $B \supset A$)
          \item Alternatively: Some people prefer to use $A \subseteq B$ to denote
                ``A is a subset of B''.
        \end{itemize}
  \item \textbf{Equality}:
        \begin{itemize}
          \item[$\bullet$] $A = B$: A and B contain exactly the same elements.
          \item[$\bullet$] $A \neq B$
        \end{itemize}
  \item \textbf{Proper subset}:
        \begin{itemize}
          \item Used in this book: $A \subsetneq B$: $A \subset B$ and $A \neq B$.
          \item Alternatively: Some people prefer to use $A \subset B$ to denote
                ``A is a proper subset of B''. Note this can be confused with the
                notation of ``subset'' so consistency of usage is important.
        \end{itemize}
  \item \textbf{Set building notation}: $\{x \in A: P(x)\}$ refers to a subset
        of the set $A$ containing all the elements of $A$ that satisfy the property
        $P(x)$.
  \item \textbf{Natural numbers}: $\mathbb{N} := \{1, 2, 3, \ldots \}$
  \item \textbf{Integers}: $\mathbb{Z} := \{0, -1, 1, -2, 2, \ldots \}$
  \item \textbf{Rational numbers}: $\mathbb{Q} := \{\frac{m}{n}: m,n \in
          \mathbb{Z}, n \neq 0 \}$
  \item \textbf{Real numbers}: $\mathbb{R}$
  \item \textbf{Union}: $A \cup B := \{x: x \in A \ or \ x \in B\}$
  \item \textbf{Intersection}: $A \cap B := \{x: x \in A \ and \ x \in B\}$
  \item \textbf{Complement}:
        \begin{itemize}
          \item $A \setminus B := \{x: x \in A \ and \ x \notin B\}$
          \item $B^c$: If the set $A$ can be understood from the context. Here ``c''
                is the first letter of the word ``complement'', not a set $c$.
        \end{itemize}
  \item \textbf{Disjoint}: $A$ and $B$ are said to be \textit{disjoint} if $A
          \cap B = \emptyset$.
  \item \textbf{Union of an infinite collection of sets}:
        \[
          \bigcup_{n=1}^{\infty} A_n := \{x: x \in A_i, \exists i \in \mathbb{N}\}
        \]
        Note the use of $\exists$: Because this is the union of the sets $A_i$,
        by definition, it means that for any element $x$ in the final union, $x$
        belongs to at least one of the sets $A_i$, but it may or may not belong to
        all of them. Therefore, when we describe the membership of $x$, we must
        express the meaning of ``may or may not''.
  \item \textbf{Intersection of an infinite collection of sets}:
        \[
          \bigcap_{n=1}^{\infty} A_n := \{x: x \in A_i, \forall i \in \mathbb{N}\}
        \]
        The use of $\forall$ means that any element $x$ in the final set belongs
        to all of the sets $A_i$.
  \item \textbf{Unions/Intersection of sets with two indices}:
        \[
          \bigcup_{n=1}^{\infty} \bigcup_{m=1}^{\infty} A_{n,m} =
          \bigcup_{n=1}^{\infty} \Bigl(\bigcup_{m=1}^{\infty} A_{n,m}\Bigr)
        \]
        \[
          \bigcap_{n=1}^{\infty} \bigcap_{m=1}^{\infty} A_{n,m} =
          \bigcap_{n=1}^{\infty} \Bigl(\bigcap_{m=1}^{\infty} A_{n,m}\Bigr)
        \]
  \item \textbf{A more general notation}:
        \[
          \bigcup_{\lambda \in I} A_{\lambda} :=
          \{x: x \in A_{\lambda}, \exists \lambda \in I\}
        \]
        \[
          \bigcap_{\lambda \in I} A_{\lambda} :=
          \{x: x \in A_{\lambda}, \forall \lambda \in I\}
        \]
\end{itemize}

\textbf{Theorem 0.3.5 (DeMorgan)}: Let $A$ be the \textit{universe} and $B, C$
the subsets of $A$. Then:

\begin{itemize}
  \item $(B \cup C)^c$ = $B^c \cap C^c$
  \item $(B \cap C)^c$ = $B^c \cup C^c$
\end{itemize}

or, more generally,

\begin{itemize}
  \item $A \setminus (B \cup C)$ = $(A \setminus B) \cap (A \setminus C)$
  \item $A \setminus (B \cap C)$ = $(A \setminus B) \cup (A \setminus C)$
\end{itemize}

% ******************************
\subsection{Induction}
% ******************************

% ------------------------------
\subsubsection{well ordering property of $\mathbb{N}$}
% ------------------------------

Every nonempty subset of $\mathbb{N}$ has a least (smallest) element.

The textbook says ``We take this property as an \textbf{axiom}; we simply
assume it is true.'' So that means we can't (or may not want to) prove this
property.

% ------------------------------
\subsubsection{Theorem 0.3.6: Principle of induction}
% ------------------------------

Let $P(n)$ be a statement depending on a natural number $n$. Suppose that:

\begin{enumerate}
  \item (basis statement) $P(1)$ is true.
  \item (induction step) If $P(n)$ is true, then $P(n+1)$ is true.
\end{enumerate}

Then $P(n)$ is true for all $n \in \mathbb{N}$.

The textbook [1] offers the following proof:

\begin{displayquote}
  Suppose $S$ is the set of natural numbers $m$ for which $P(m)$ is not true.
  Suppose $S$ is nonempty. Then $S$ has a least element by the well ordering
  property. Let us call $m$ the least element of S. We know $1 \notin S$ by
  assumption. So $m > 1$ and $m - 1$ is a natural number as well. Since $m$ is
  the least element of $S$, we know that $P(m-1)$ is true. But by the induction
  step we see that $P(m-1+1) = P(m)$ is true, contradicting the statement that
  $m \in S$. Therefore, $S$ is empty and $P(n)$ is true for all $n \in
    \mathbb{N}$.
\end{displayquote}

My notes on the proof:

\begin{itemize}
  \item The proof uses contradiction. Therefore, we need to suppose the opposite
        conclusion of the theorem is true. Because the conclusion of the theorem is
        ``$P(n)$ is true for \textbf{all} $n \in \mathbb{N}$'', the opposite
        conclusion is ``there exist some $m \in \mathbb{N}$ that $P(m)$ is false''.
        This is why the first sentence says ``... $m$ for which $P(m)$ is not true''
        and ``$S$ is nonempty'': $S$ must not be empty, because otherwise the
        original conclusion is true and it makes no sense to use proof by
        contradiction.
  \item Regarding $1 \notin S$: The basis statement is $P(1)$ is true; $S$ is
        the set of natural numbers $m$ where $P(m)$ is false. Therefore, if $1 \in
          S$, that means $P(1)$ is false, which contradicts with the conditions of
        the theorem to-be-proven. Note that ``proof by contradiction'' starts with
        the opposite \textbf{conclusion} of the original theorem but it still
        accepts all the conditions of the original theorem. In this case, ``proof
        by contradiction'' still accepts the condition that $P(1)$ is true.
\end{itemize}

% ------------------------------
\subsubsection{Theorem 0.3.9: Principle of strong induction}
% ------------------------------

Let $P(n)$ be a statement depending on a natural number $n$. Suppose that:

\begin{enumerate}
  \item (basis statement) $P(1)$ is true.
  \item (induction step) If $P(k)$ is true for all $k = 1, 2, \ldots, n$, then
        $P(n+1)$ is true.
\end{enumerate}

Then $P(n)$ is true for all $n \in \mathbb{N}$.

The textbook leaves the proof of this theorem as an exercise, so here is mine:

My proof: We still use the method of ``proof by contradiction''. Suppose $S$ is
the set of natural numbers $i$ for which $P(i)$ is not true and $S$ is not
empty. By the well ordering property, we know $S$ has a least element and let's
call it $m$. Because $P(k)$ is true for all $k = 1, 2, \ldots, n$, we know that
$1, 2, \ldots, n \notin S$. Therefore, $m > n$ and $m-1$ is a natural number,
too. Since $m$ is the least element of $S$, we know $m-1 \notin S$. Because
$P(i)$ is not true for all $i \in S$ and because $m-1 \notin S$, we know
$P(m-1)$ is true. However, by the induction step, we see that $P(m-1+1) = P(m)$
must be true, so $m \notin S$, contradicting the statement that $m \in S$.
Therefore, $S$ must be empty and $P(n)$ is true for all $n \in \mathbb{N}$.

% ******************************
\subsection{Functions}
% ******************************

In order to define functions rigorously, we must define the Cartesian product
firstly.

% ------------------------------
\subsubsection{Definition 0.3.10 Cartesian product}
% ------------------------------

Let $A$ and $B$ be sets. The \textit{Cartesian product} is the set of tuples
defined as \[ A \times B := \{(x, y): x \in A, y \in B \} \].

\textbf{Note}: ``tuple'' is the same as the ``list'' as we see in ``Linear
Algebra Done Right''.

Examples:
\begin{itemize}
  \item $[0,1] \times [0,1]$ = $\{(0,0), (0,1), (1,0), (1,1)\}$
  \item $[0,1] \times [0,1]$ is also denoted as $[0,1]^2$.
  \item $\mathbb{R}^2$ = $\mathbb{R} \times \mathbb{R}$
\end{itemize}

% ------------------------------
\subsubsection{Definition 0.3.11 Function}
% ------------------------------

A \textit{function} $f: A \rightarrow B$ is a subset $f$ of $A \times B$ such
that for each $x \in A$, there is a \textbf{unique} $(x,y) \in f$. We then
write $f(x) = y$.

\colorbox{lime}{NOTE(ywen)}: This word ``unique'' does exclude the case of the
same $x$ being mapped to different $y_i$, but it doesn't exclude the case of
different $x_i$ being mapped to the same $y$.

The various terms that are related with functions:
\begin{itemize}
  \item Sometimes $f$ is called a \textbf{mapping} or a \textbf{map}: $f$
        \textbf{maps} $A$ to $B$.
  \item Sometimes the set $f$ is called the \textbf{graph} of the function
        rather than the function itself.
  \item \textbf{domain}: The set $A$ is called the domain of $f$. Sometimes it
        is denoted as $D(f)$.
  \item \textbf{range}: The set $R(f) := \{y \in B: \exists x: f(x) = y\}$ is
        called the range of $f$.
\end{itemize}

% ------------------------------
\subsubsection{Definition 0.3.13 Direct image and inverse image}
% ------------------------------

Consider a function $f: A \rightarrow B$ and $C \subset A$. Define the
\textit{image} (or \textit{direct image}) of $C$ as \[ f(C) := \{f(x) \in B:
  x \in C\} \].

Let $D \subset B$. Define the \textit{inverse image} of $D$ as \[ f^{-1}(D) :=
  \{x \in A: f(x) \in D\}\].

\textbf{Note} the definition of inverse image: the elements in $D$ do not have
to have an element in $A$ that maps to it. See the example $f^{-1}(\{a,b,c\})$
in \textbf{Figure 3} in the textbook.

% ------------------------------
\subsubsection{Proposition 0.3.15}
% ------------------------------

Consider $f: A \rightarrow B$. Let $C$, $D$ be subsets of $B$. Then:
\begin{enumerate}
  \item $f^{-1}(C \cup D)$ = $f^{-1}(C) \cup f^{-1}(D)$
  \item $f^{-1}(C \cap D)$ = $f^{-1}(C) \cap f^{-1}(D)$
  \item $f^{-1}(C^c)$ = $\bigl(f^{-1}(C)\bigr)^c$ (which means $f^{-1}(B
          \setminus C) = A \setminus f^{-1}(C)$)
\end{enumerate}

The textbook offers the proof for the first statement, but I want to prove it
with more details.

\colorbox{lime}{\textbf{My proof}}:

To prove that two sets are equal, we need to use the definition of \textit{
  equal} in definition 0.3.2, that two sets $A$ and $B$ are equal if $A \subset B$
and $B \subset A$.

\begin{enumerate}
  \item The proof is divided into three steps:
        \begin{enumerate}
          \item $\forall x \in f^{-1}(C \cup D)$, according to the definition of
                $f^{-1}$, we know $f(x) \in C \cup D$. According to the definition of
                \textit{union}, we know this means $f(x) \in C$ or $f(x) \in D$. When
                $f(x) \in C$, this means $x \in f^{-1}(C)$; when $f(x) \in D$, this
                means $x \in f^{-1}(D)$. According to the definition of \textit{union}
                again, we know this means $x \in f^{-1}(C) \cup f^{-1}(D)$. Therefore,
                the whole reasoning chain can be summarized as $\forall x \in f^{-1}(C
                  \cup D) \Rightarrow x \in f^{-1}(C) \cup f^{-1}(D)$. According to the
                definition of \textit{subset}, this means $f^{-1}(C \cup D) \subset
                  f^{-1}(C) \cup f^{-1}(D)$.
          \item $\forall x \in f^{-1}(C) \cup f^{-1}(D)$ means $x \in f^{-1}(C)$ or
                $x \in f^{-1}(D)$ which means $f(x) \in C$ or $f(x) \in D$ which means
                $f(x) \in C \cup D$. According to the definition of \textit{inverse
                  image}, this means $x \in f^{-1}(C \cup D)$. Therefore, the whole
                reasoning chain can be summarized as $\forall x \in f^{-1}(C) \cup
                  f^{-1}(D) \Rightarrow x \in f^{-1}(C \cup D)$. According to the
                definition of \textit{subset}, this means $f^{-1}(C) \cup f^{-1}(D)
                  \subset f^{-1}(C \cup D)$.
          \item According to the definition of \textit{equal}, we know $f^{-1}(C
                  \cup D)$ = $f^{-1}(C) \cup f^{-1}(D)$.
        \end{enumerate}
  \item The proof is divided into three steps:
        \begin{enumerate}
          \item $\forall x \in f^{-1}(C \cap D)$, according to the definition of
                $f^{-1}$, we know $f(x) \in C \cap D$. According to the definition of
                \textit{intersection}, we know this means $f(x) \in C$ and $f(x) \in D$.
                When $f(x) \in C$, this means $x \in f^{-1}(C)$; when $f(x) \in D$, this
                means $x \in f^{-1}(D)$. According to the definition of \textit{
                  intersection} again, we know this means $x \in f^{-1}(C) \cap
                  f^{-1}(D)$. Therefore, the whole reasoning chain can be summarized as
                $\forall x \in f^{-1}(C \cap D) \Rightarrow x \in f^{-1}(C) \cap
                  f^{-1}(D)$. According to the definition of \textit{subset}, this means
                $f^{-1}(C \cap D) \subset f^{-1}(C) \cap f^{-1}(D)$.
          \item $\forall x \in f^{-1}(C) \cap f^{-1}(D)$ means $x \in f^{-1}(C)$ and
                $x \in f^{-1}(D)$ which means $f(x) \in C$ and $f(x) \in D$ which means
                $f(x) \in C \cap D$. According to the definition of \textit{inverse
                  image}, this means $x \in f^{-1}(C \cap D)$. Therefore, the whole
                reasoning chain can be summarized as $\forall x \in f^{-1}(C) \cap
                  f^{-1}(D) \Rightarrow x \in f^{-1}(C \cap D)$. According to the
                definition of \textit{subset}, this means $f^{-1}(C) \cap f^{-1}(D)
                  \subset f^{-1}(C \cap D)$.
          \item According to the definition of \textit{equal}, we know $f^{-1}(C
                  \cap D)$ = $f^{-1}(C) \cap f^{-1}(D)$.
        \end{enumerate}
  \item The proof is divided into three steps:
        \begin{enumerate}
          \item First of all, according to the definition of \textit{inverse image}
                on $f: A \rightarrow B$, we know that $\forall x \in f^{-1}(B \setminus
                  C)$ comes to two conclusions:
                \begin{enumerate}
                  \item $x \in A$
                  \item $f(x) \in B \setminus C$
                \end{enumerate}
                The second conclusion means $f(x) \in B$ and $f(x) \notin C$. When $f(x)
                  \notin C$, we know $x \notin f^{-1}(C)$ (because if $x \in f^{-1}(C)$,
                by definition it means $f(x) \in C$, contradicting with the condition
                that $f(x) \notin C$). Therefore, now we have the following two
                conditions hold at the same time:
                \begin{enumerate}
                  \item $x \in A$
                  \item $x \notin f^{-1}(C)$
                \end{enumerate}
                This means $x \in A \setminus f^{-1}(C)$. The whole reasoning chain is
                \[
                  \forall x \in f^{-1}(B \setminus C) \Rightarrow x \in A \setminus
                  f^{-1}(C)
                \]
                So we know $f^{-1}(B \setminus C) \subset (A \setminus f^{-1}(C))$.
          \item Secondly, $\forall x \in A \setminus f^{-1}(C)$, we come to two
                conclusions:
                \begin{enumerate}
                  \item $x \in A \Rightarrow f(x) \in B$.
                  \item $x \notin f^{-1}(C) \Rightarrow f(x) \notin C$.
                \end{enumerate}
                Therefore, we know $f(x) \in (B \setminus C)$, so $x \in f^{-1}(B
                  \setminus C)$. The whole
                reasoning chain is
                \[
                  \forall x \in A \setminus f^{-1}(C) \Rightarrow x \in f^{-1}(B
                  \setminus C)
                \]
                So we know $A \setminus f^{-1}(C) \subset f^{-1}(B \setminus C)$
          \item According to the definition of \textit{equal}, we know $f^{-1}(B
                  \setminus C)$ = $A \setminus f^{-1}(C)$.
        \end{enumerate}
\end{enumerate}

% ------------------------------
\subsubsection{Proposition 0.3.16}
% ------------------------------

Consider $f: A \rightarrow B$. Let $C$, $D$ be subsets of $A$. Then
\begin{enumerate}
  \item $f(C \cup D)$ = $f(C) \cup f(D)$
  \item $f(C \cap D) \subset f(C) \cap f(D)$
\end{enumerate}

To prove 1, we need to use the following conclusion: Given $f: A \rightarrow B$.
Let $C$, $D$ be subsets of $A$. Then \[f(x) \in f(C) \cup f(D) \Rightarrow
  x \in C \cup D\]

We can use contradiction to prove this. Suppose $\exists u \notin (C \cup D)$
such that $f(u) \in f(C) \cup f(D)$. Because $C$ and $D$ are discussed under
the universe $A$, we have $u \notin (C \cup D)$, which means $u \in A \setminus
  (C \cup D)$, which means $u \in (A \setminus C)$ AND $u \in (A \setminus D)$,
which means $u \notin C$ AND $u \notin D$.

When $u \notin C$, we can tell that $f(u) \notin f(C)$ (because otherwise if
$f(u) \in f(C)$, according to the definition of \textit{direct image}, $u$ must
be an element of $C$, contradicting the precondition $u \notin C$). Similarly,
we can tell that when $u \notin D$, $f(u) \notin f(D)$.

Because $\forall x \in A$, we have $f(x) \in B$, so $f(C)$ and $f(D)$ must be
subsets of $B$, i.e., $B$ is the universe of $C$ and $D$. Therefore, when we
say $f(u) \notin f(C)$, we mean $f(u) \in B \setminus f(C)$; when we say $f(u)
  \notin f(D)$, we mean $f(u) \in B \setminus f(D)$. Because $f(u) \notin f(C)$
AND $f(u) \notin f(D)$, we have $f(u) \in \bigl(B \setminus f(C)\bigr) \cap
  \bigl(B \setminus f(D)\bigr) = B \setminus \bigl(f(C) \cup f(D)\bigr)$, so
$f(u) \in B$ but $f(u) \notin \bigl(f(C) \cup f(D)\bigr)$ which contradicts
the precondition that $f(u) \in \bigl(f(C) \cup f(D)\bigr)$. So such a $u
  \notin (C \cup D)$ does \textbf{not} exist.

\textbf{My proof of 1}:

To prove $f(C \cup D)$ = $f(C) \cup f(D)$, we need to prove $f(C \cup D)
  \subset f(C) \cup f(D)$ and $f(C \cup D) \supset f(C) \cup f(D)$.

According to the definition of \textit{direct image}, $f(C \cup D) := \{f(x)
  \in B: x \in C \cup D\}$, so $\forall f(x) \in f(C \cup D)$, we have $x \in C
  \cup D$, so $x \in C$ or $x \in D$. When $x \in C$, $f(x) \in f(C) := \{f(x): x
  \in C\}$; when $x \in D$, $f(x) \in f(D) := \{f(x): x \in D\}$. Therefore, $f(x)
  \in f(C)$ or $f(x) \in f(D)$ depending on $x \in C$ or $x \in D$, i.e., when $x
  \in C \cup D$, $f(x) \in B$, so we know all the $f(x)$ when $x \in C \cup D$ is
defined as $f(C \cup D)$. So $\forall f(x) \in f(C) \cup f(D)$, we have $f(x)
  \in f(C \cup D)$. So $f(C \cup D) \subset f(C \cup D)$.

On the other way, $\forall f(x) \in f(C) \cup f(D)$ $\Rightarrow$ $f(x) \in
  f(C)$ or $f(x) \in f(D)$. According to the definition of \textit{direct image},
\begin{itemize}
  \item $f(C) := {f(x) \in B: x \in C}$
  \item $f(D) := {f(x) \in B: x \in D}$.
\end{itemize}

Therefore:
\begin{itemize}
  \item When $f(x) \in f(C)$, we know $x \in C$.
  \item When $f(x) \in f(D)$, we know $x \in D$.
\end{itemize}

Therefore, when $f(x) \in f(C) \cup f(D)$, we have $x \in C$ or $x \in D$, i.e.,
$x \in C \cup D$. According to the definition of \textit{direct image}, when $x
  \in C \cup D$, $f(x)$ is defined as $f(C \cup D)$. So we know $f(x) \in f(C
  \cup D)$. So $\forall f(x) \in f(C) \cup f(D)$, we have $f(x) \in f(C \cup D)$.
So $f(C \cup D) \subset f(C \cup D)$.

To sum up, we have $f(C \cup D) \subset f(C) \cup f(D)$ and $f(C \cup D)
  \supset f(C) \cup f(D)$, so $f(C \cup D)$ = $f(C) \cup f(D)$.

\textbf{My proof of 2}:

According to the definition of \textit{direct image}, $f(C \cap D) := \{f(x)
  \in B: x \in C \cap D\}$, so $\forall f(x) \in f(C \cap D)$, we have $x \in C
  \cap D$, so $x \in C$ and $x \in D$.

\begin{itemize}
  \item When $x \in C$, we have $f(x) \in f(C)$.
  \item When $x \in D$, we have $f(x) \in f(D)$.
\end{itemize}

Because $x \in C$ and $x \in D$, we have $f(x) \in f(C)$ and $f(x) \in f(D)$,
namely $f(x) \in f(C) \cap f(D)$.

In sum, we have $\forall f(x) \in f(C \cap D)$, $f(x) \in f(C) \cap f(D)$, so
we have $f(C \cap D) \subset f(C) \cap f(D)$.

\textbf{The other way may not hold}, i.e., $f(C) \cap f(D)$ may \textbf{not} be
a subset of $f(C \cap D)$. A counter example is $f: x \rightarrow x^2, x \in
  \mathbb{R}$. Here, in the context of this proposition, $A$ and $B$ are both
$\mathbb{R}$. Now let $C := [-2, 0]$ and $D := [0, 2]$. We have:

\begin{itemize}
  \item $f(C) = [0, 4]$
  \item $f(D) = [0, 4]$
  \item $f(C) \cap f(D) = [0, 4]$
  \item $C \cap D = \{0\}$, so $f(C \cap D) = f(\{0\}) = [0]$
\end{itemize}

Obviously, $f(C) \cap f(D)$ is \textbf{not} a subset of $f(C \cap D)$.

\colorbox{red!100}{\textcolor{yellow}{TODO}}: Prove the other way in a general
way (i.e., not using counter examples).

% ------------------------------
\subsubsection{Definition 0.3.17 (injective; surjective; bijective; inverse function)}
% ------------------------------

Let $f: A \rightarrow B$ be a function. The function $f$ is said to be
\textit{injective} or \textit{one-to-one} if $f(x_1) = f(x_2)$ implies $x_1 =
  x_2$. In other words, $\forall y \in B$, $f^{-1}(\{y\})$ is empty or consists
of a single element. We call such an $f$ an \textit{injection}.

If $f(A) = B$, then we say $f$ is \textit{surjective} or \textit{onto}. We call
such an $f$ a \textit{surjection}.

If $f$ is both an surjection and injection, then we say $f$ is
\textit{bijective} or that $f$ is a \textit{bijection}.

When $f: A \rightarrow B$ is a bijection, then the inverse image of a single
element, $f^{-1}(\{y\})$, is always a unique element of A. We then consider
$f^{-1}$ as a function $f^{-1}: B \rightarrow A$ and we write simply
$f^{-1}(y)$. In this case, we call $f^{-1}$ the \textit{inverse function} of
$f$.

% ------------------------------
\subsubsection{Definition 0.3.18 (composition)}
% ------------------------------

Consider $f: A \rightarrow B$ and $g: B \rightarrow C$. The \textit{composition}
of the functions $f$ and $g$ is the function $g \circ f: A \rightarrow C$
defined as \[(g \circ f)(x) := g \bigl(f(x)\bigr)\].

\colorbox{lime}{NOTE(ywen)}: The functions $f$ and $g$ may or may not be
injective or surjective.

% ******************************
\subsection{Relations and equivalence classes}
% ******************************

% ------------------------------
\subsubsection{Definition 0.3.19 (binary relation)} \label{def:binary-relation}
% ------------------------------

Given a set $A$, a \textit{binary relation} on $A$ is a subset $\mathcal{R}
  \subset A \times A$, which are those pairs where the relation is said to hold.
Instead of $(a, b) \subset \mathcal{R}$, we write $a \mathcal{R} b$.

Any subset of $A \times A$ is a relation. See \textbf{Example 0.3.20}.

% ------------------------------
\subsubsection{Definition 0.3.21 (reflexive; symmetric; transitive; equivalence)}
% ------------------------------

Let $\mathcal{R}$ be a relation on a set $A$. Then $\mathcal{R}$ is said to be
\begin{enumerate}
  \item[(i)] \textit{reflexive} if $\forall a \in A$, $a \mathcal{R} a$,
  \item[(ii)] \textit{symmetric} if $a \mathcal{R} b \Rightarrow b \mathcal{R}
          a$,
  \item[(iii)] \textit{transitive} if $a \mathcal{R} b$ and $b \mathcal{R} c$
        implies $a \mathcal{R} c$.
\end{enumerate}

If $\mathcal{R}$ is reflexive, symmetric, and transitive, then it is said to be
an \textit{equivalence relation}.

% ------------------------------
\subsubsection{Definition 0.3.23 (equivalence class)}
% ------------------------------

Let $A$ be a set and $\mathcal{R}$ an equivalence relation. An
\textit{equivalence class} of $a \in A$, often denoted by $[a]$, is the set
$\{x \in A: a \mathcal{R} x\}$.

\begin{itemize}
  \item Reflexivity guarantees that $a \in [a]$.
  \item Symmetry guarantees that if $b \in [a]$, then $a \in [b]$.
  \item Transitivity guarantees that if $a \in [b]$ and $b \in [c]$, then $a
          \in [c]$.
\end{itemize}

% ------------------------------
\subsubsection{Proposition 0.3.24 (equivalence class)}
% ------------------------------

If $\mathcal{R}$ is an \textit{equivalence relation} on a set $A$, then every
$a \in A$ is in exactly one equivalence class. In particular, $a \mathcal{R} b$
if and only if $[a] = [b]$.

\colorbox{lime}{\textbf{My proof}}: Because the entire proposition consists of
a few parts, we need to prove each part separately.

\textbf{(1)} Prove that ``every $a \in A$ is in some equivalence class.''
Because $\mathcal{R}$ is an equivalence relation, it must be reflexive too, so
$\forall a \in A$, $a \mathcal{R} a$ must hold. Therefore, $a$ itself must be
in $[a]$ at least.

\textbf{(2)} Prove that ``every $a \in A$ has no more than one equivalence
class.'' Suppose $a$ has at least two equivalence classes, denoted as $M := \{
  m \in A: a \mathcal{R} m\}$ and $N := \{n \in A: a \mathcal{R} n\}$. We have:

\begin{itemize}
  \item $\forall u \in M$, we have $a \mathcal{R} u$, so $u$ must belong to N
        as well, so we have $M \subset N$.
  \item $\forall w \in N$, we have $a \mathcal{R} w$, so $w$ must belong to M
        as well, so we have $N \subset M$.
\end{itemize}

Therefore, $M = N$, so $a \in A$ has no more than one equivalence class.

\textbf{(3)} Prove that $a \mathcal{R} b$ $\Rightarrow$ $[a] = [b]$.

According to the definition of equivalence class, we have: \[[a] := \{x \in A:
  a \mathcal{R} x\}\] and \[[b] := \{x \in A: b \mathcal{R} x\}\].

Because $\mathcal{R}$ is an equivalence relation, it must be symmetric, i.e.,
$a \mathcal{R} b$ $\Rightarrow$ $b \mathcal{R} a$. Therefore:
\begin{enumerate}
  \item $\forall x \in [a]$, we have $a \mathcal{R} x$.
  \item Put $a \mathcal{R} x$, $b \mathcal{R} a$, and the transitive property
        of $mathcal{R}$ together, we have $b \mathcal{R} x$.
  \item Therefore, $x \in [b]$.
  \item Therefore, $[a] \subset [b]$.
\end{enumerate}

Similarly,
\begin{enumerate}
  \item $\forall x \in [b]$, we have $b \mathcal{R} x$.
  \item Put $b \mathcal{R} x$, $a \mathcal{R} b$, and the transitive property
        of $mathcal{R}$ together, we have $a \mathcal{R} x$.
  \item Therefore, $x \in [a]$.
  \item Therefore, $[b] \subset [a]$.
\end{enumerate}

In sum, $[a] = [b]$.

\textbf{(4)} Prove that $[a] = [b]$ $\Rightarrow$ $a \mathcal{R} b$.

According to the definition of equivalence class, we have: \[[a] := \{x \in A:
  a \mathcal{R} x\}\] and \[[b] := \{x \in A: b \mathcal{R} x\}\].

Because $[a] = [b]$, $\forall x \in [a]$, we have $x \in [b]$, so $a
  \mathcal{R} x$ and $b \mathcal{R} x$ must hold at the same time. Because
$\mathcal{R}$ is an equivalence relation, it must be symmetric, so
$b \mathcal{R} x$ $\Rightarrow$ $x \mathcal{R} b$. Together with
$a \mathcal{R} x$ and the transitive property of $\mathcal{R}$, we have
$a \mathcal{R} b$.

In sum, (1) + (2) imply that ``every $a \in A$ is in exactly one equivalence
class''; (3) + (4) imply that ``$a \mathcal{R} b$ if and only if $[a] = [b]$''.

% ------------------------------
\subsubsection{Example 0.3.25}
% ------------------------------

The set of rational numbers can be defined as equivalence classes of a pair of
an integer and a natural number, that is elements of $\mathbb{Z} \times
  \mathbb{N}$. The relation is defined by $(a, b) \sim (c, d)$ whenever $ad = bc$.

Prove $\sim$ is an equivalence relation.

\colorbox{lime!100}{\textbf{My proof}}: Let $A$ denote $\mathbb{Z} \times
  \mathbb{N}$, so $A := \{(z, n): z \in \mathbb{Z}, n \in \mathbb{N}\}$.

\begin{itemize}
  \item \textit{reflexive}: $\forall (z, n) \in A$, $\because zn = zn$,
        $\therefore (z, n) \sim (z, n)$.
  \item \textit{symmetric}: $\forall (z, n), (x, y) \in A$, when $(z, n) \sim
          (x, y)$, we know $zy = nx$, $\therefore nx = zy$. Because $z, n, x, y \in
          \mathbb{Z}$ (remember $\mathbb{N} \subset \mathbb{Z}$), therefore $nx = xn$
        and $zy = yz$, so $nx = zy \Rightarrow xn = yz$, so $(x, y) \sim (z, n)$
        also holds. In sum, $(z, n) \sim (x, y)$ implies $(x, y) \sim (z, n)$.
  \item \textit{transitive}: Suppose $(z, n) \sim (p, q)$ and $(p, q) \sim
          (x, y)$. Because $(z, n) \sim (p, q)$, we know $zq = pn$; because $n, q \in
          \mathbb{N}$ so they can't be zero, we can divide both sides by $nq$ to get
        $z/n = p/q$. Similarly, we can get $p/q = x/y$. Therefore, we can get $z/n
          = p/q$. Therefore, $zq = np$, so $(z, n) \sim (p, q)$. In sum, that $(z, n)
          \sim (p, q)$ and $(p, q) \sim (x, y)$ implies $(z, n) \sim (x, y)$.
\end{itemize}

% ******************************
\subsection{Cardinality}
% ******************************

% ------------------------------
\subsubsection{Definition 0.3.26 (cardinality)}
% ------------------------------

Let $A$ and $B$ be sets. We say $A$ and $B$ have the same \textit{cardinality}
when there exists a bijection $f: A \rightarrow B$. We denote by $|A|$ the
equivalence class of all sets with the same cardinality as A and we simply call
$|A|$ the cardinality of $A$.

\colorbox{lime!100}{\textbf{My notes}}:
\begin{itemize}
  \item Note that, technically, $|A|$ is ``the equivalence class of all sets
        with the same cardinality as A'', so $|A|$ is a set. Every element in $|A|$
        is also a set, so $|A|$ is a set of sets.
  \item $|A|:= \{S: S \sim A\}$ where the binary relation $\sim$ is defined so
        that $S_1 \sim S_2$ means the sets $S_1$ and $S_2$ have the same
        cardinality.
\end{itemize}

% ------------------------------
\subsubsection{Definition 0.3.27 ($|A|$; finite; infinite)}
% ------------------------------

Suppose $A$ has the same cardinality as $\{1, 2, 3, \ldots , n\}$ for some $n
  \in \mathbb{N}$. We then write $|A|:= n$. If $A$ is empty, we write $|A|:= 0$.
In either case, we say that $A$ is \textit{finite}.

We say $A$ is \textit{infinite} or ``of infinite cardinality'' if $A$ is not
finite.

The textbook then asks the readers to prove that for each nonempty finite set
$A$, there exists a unique natural number $n$ such that there exists a bijection
from $A$ to $\{1, 2, 3, \ldots, n\}$.

\colorbox{lime!100}{\textbf{My proof}}: Because $A$ is finite, we can enumerate
all of its elements in a certain order. For example, if $A$ is the set of
English lower-case alphabet, i.e., $\{a, b, c, \ldots, z\}$, then we can arrange
them in the order they appear in the alphabet and give each lower-case letter an
index:
\begin{itemize}
  \item a $\rightarrow$ 1
  \item b $\rightarrow$ 2
  \item c $\rightarrow$ 3
  \item $\ldots$
  \item z $\rightarrow$ 26
\end{itemize}

Even for a set of objects (e.g., fruits) or symbols (e.g., $\{\clubsuit,
  \diamondsuit, \heartsuit, \spadesuit \}$), as long as it is a finite number of
items, we can always arrange them in a certain order. A more general algorithm
to arrange a finite set $S$ can be described as follows:

\begin{pseudocode}[ruled]{EnumerateSetElements}{S}
  \LOCAL{T, i} \\
  T \GETS \emptyset \\
  i \GETS 1 \mbox{/* Next index of the picked element from S. */} \\
  \\
  \WHILE \NOT (S = \emptyset) \DO
  \BEGIN
  s \GETS \mbox{pick a random element from S} \\
  t_i \GETS s \mbox{/* $t_i$ is just an indexed symbol to denote s. */} \\
  S \GETS S \setminus \{s\} \\
  T \GETS T \cup \{t_i\} \\
  i \GETS i+1
  \END \\
  \\
  \mbox{/* c is the number of elements in T. */} \\
  c \GETS i - 1 \\
  \\
  \RETURN{T, c}
\end{pseudocode}

After this algorithm is run on the set $S$, we have the following conclusions:
\begin{itemize}
  \item Because $S$ is a finite set, $c$ must be an integer in $\mathbb{N}$.
  \item $T:= \{t_i: i \in \mathbb{N}, i \leq c, t_i \in S \}$
  \item $T = S$ because all the elements in $T$ come from $S$.
\end{itemize}

Now we define $C := \{i: i \in \mathbb{N}, i \leq c\}$. Then we define
$f: T \rightarrow C$ as \[\{(t_i, i): i \in \mathbb{N}, i \leq c\}\] i.e., \[
  f(t_i) = i, i \in \mathbb{N}, i \leq c\].

Now we need to prove that $f$ is bijective and $c$ is unique.

Firstly, let's see whether $f$ is injective. $\forall p, q \in N$, according to
the definition of $f$, we know $p = f(t_p)$ and $q = f(t_q)$. When $p = q$, we
can tell that $t_p$ and $t_q$ must point to the same element at the $p$-th (or
$q$-th) location in $T$. Therefore, $t_p = t_q$. So $f$ is injective.

Secondly, let's see whether $f$ is surjective. Suppose $\exists w \in C$ such
that $\forall t \in T, f(t) \neq w$. Because $w \in C$, therefore $t_w \in T$,
therefore $f(t_w) = w$, contradicting that $\forall t \in T, f(t) \neq w$, so
such a $w$ doesn't exist in $N$.

In sum, because $f$ is both injective and surjective, it is thus bijiective.

$c$ must be unique. Suppose there exists $c_2$ such that $c \neq c_2$.
Define $C_2:= \{i: i \in \mathbb{N}, i \leq c_2\}$. Suppose we can find two
bijective functions $g$ and $h$ so that $g: T \rightarrow C$ and $h: T
  \rightarrow C_2$. According to definition 0.3.26, we know that $A$ and $C$ have
the same cardinality and $A$ and $C_2$ have the same cardinality. According to
definition 0.3.27, we can write $|A| = c$ and $|A| = c_2$, so $c = c_2$,
contradicting that $c \neq c_2$. Therefore, such an $c_2$ doesn't exist. So $c$
is unique.

% ------------------------------
\subsubsection{Note: $\mathbb{N}$ is infinite}
% ------------------------------

\colorbox{lime!100}{\textbf{My notes}}: We take for granted that $\mathbb{N}$
is infinite. But how can we prove it?

According to the definition 0.3.27, we need to prove that $\mathbb{N}$ is not
finite. Because a set $A$ being finite is defined as that $A$ has the same
cardinality as the set $\{1, 2, 3, \ldots, n\}$ for some $n \in \mathbb{N}$, we
need to prove that such an $n \in \mathbb{N}$ doesn't exist for $\mathbb{N}$
itself. (The second case that $A$ is finite is that $A$ is $\emptyset$ but this
is surely not the case for $\mathbb{N}$, so we don't need to consider this.)

Firstly, let's define $N_k:= \{i: i \leq k, k \in \mathbb{N}\} = \{1, 2, 3,
  \ldots, k\}$. Now let's prove that no matter how we choose $k$, $\mathbb{N}$
doesn't have the same cardinality with $N_k$.

We use contradiction to prove this. Suppose that $\exists k \in \mathbb{N}$
such that $N_k$ and $\mathbb{N}$ have the same cardinality. According to the
definition 0.3.26, having the same cardinality means there exists a bijection
$f: N_k \rightarrow \mathbb{N}$ or $g: \mathbb{N} \rightarrow N_k$.

However, no matter how $f$ is defined, $f$ can't be a bijection because it is
not surjective, i.e., $\exists P \in \mathbb{N}$ such that $P \notin f(N_k)$.
$P$ can be found this way: Define \[K = \sum_{i=1}^{k} f(i) \] and \[P = K +
  1\].

We can tell that $\forall i \in N_k$, $P \neq f(i)$, so $P \notin f(N_k)$. But
because $\forall i \in N_k, f(i) \in \mathbb{N}$, so the sum of them is still
a natural number, i.e., $K \in \mathbb{N}$, hence $P = K + 1 \in \mathbb{N}$.
Therefore, $P \in \mathbb{N}$ but $P \notin f(N_k)$, so $f$ is not surjective,
so $f$ is not bijective.

We then examine $g$. We can tell that no matter how $g$ is define, $g$ can't be
a bijection because it is not injective. Suppose that $\forall i, j \in N_k$
and $i \neq j$, we find $e_i, e_j \in \mathbb{N}$ and $e_i \neq e_j$ such that
$g(e_i) = i$ and $g(e_j) = j$. Now define \[E = \sum_{i=1}^{k} e_i + 1\]. We
can tell that $\forall i \in N_k, E \neq e_i$. Because the sum of natural
numbers is also a natural number, so $E \in \mathbb{N}$. Suppose that $g(E) = w
  \in N_k$. According to how $g$ is defined, we can tell that $g(e_w) = w$. But
because $E \neq e_w$, we have that $f(E) = f(e_w)$ does not imply $E = e_w$, so
$g$ can't be injective, so $g$ can't be bijective.

In sum, $\forall k \in \mathbb{N}$, we can't find a bijection between $N_k$ and
$\mathbb{N}$, so $\mathbb{N}$ and $N_k$ don't have the same cardinality, so
$\mathbb{N}$ is not finite, hence infinite.

% ------------------------------
\subsubsection{Definition 0.3.28 ($|A|=|B|$;$|A| \leq |B|$;$|A|<|B|$)}
% ------------------------------

We write \[|A| \leq |B|\] if there exists an injection from $A$ to $B$.

We write \[|A| = |B|\] if $A$ and $B$ have the same cardinality.

We write \[|A| < |B|\] if $|A| \leq |B|$, but $A$ and $B$ do not have the same
cardinality.

% ------------------------------
\subsubsection{Cantor-Bernstein-Schroder theorem}
% ------------------------------

(\colorbox{red!100}{\textcolor{yellow}{TODO}}: Fix ``o'' in ``Schroder''.)

$A$ and $B$ have the same cardinality if and only if $|A| \leq |B|$ and $|B|
  \leq |A|$.

If $A$ and $B$ are any two sets, we can always write $|A| \leq |B|$ or $|B|
  \leq |A|$.

% ------------------------------
\subsubsection{Definition 0.3.29 (countable; uncountable)}
% ------------------------------

If $|A| = |\mathbb{N}|$, then $A$ is said to be \textit{countably infinite}. If
$A$ is finite or countably infinite, then we say $A$ is \textit{countable}. If
$A$ is not countable, then A is said to be \textit{uncountable}.

$|\mathbb{N}|$ is usually denoted as $\aleph_0$ (read as aleph-naught).

% ------------------------------
\subsubsection{Example 0.3.31}
% ------------------------------

$\mathbb{N} \times \mathbb{N}$ is a countably infinite set.

\colorbox{lime!100}{\textbf{My proof}}: We can figure out the formula for the
bijection that the textbook proof uses.

In the way that the textbook proof arranges the elements of $\mathbb{N} \times
  \mathbb{N}$, we can see that, $\forall (a, b) \in \mathbb{N} \times \mathbb{N}$:
\begin{itemize}
  \item $a + b = 2$: 1 element: $(1, 1)$
  \item $a + b = 3$: 2 elements: $(1, 2), (2, 1)$
  \item $a + b = 4$: 3 elements: $(1, 3), (2, 2), (3, 1)$
  \item $a + b = 5$: 4 elements: $(1, 4), (2, 3), (3, 2), (4, 1)$
  \item $\ldots$
  \item $a + b = k$: (k-1) elements: $(1, k-1), (2, k-2), $\ldots$, (k-1, 1)$
\end{itemize}

We can use the sum of $a$ and $b$ to name that group of elements. For example,
$(1, 1)$ is in the group of $2$; $(1, 2)$ and $(2, 1)$ are in the group of $3$.
We can easily tell that the group $a + b$ has $(a + b -1)$ elements. Let's
define $NUM$ as the function that, given $a + b$, returns the number of elements
in this group: \[NUM(a + b) = a + b -1\]

Therefore, when we are looking at an element $(a, b)$, we know it is in the
group of $a + b$, and $(a, b)$ is the $a$-th element in this group. The total
number of elements in all the groups \textbf{before} group $a + b$ is:

\begin{equation*}
  \begin{split}
    \sum_{i=2}^{a + b - 1} NUM(i)
    & = 1 + 2 + 3 + \ldots + (a + b - 1 - 1) \\
    & = \frac{1}{2}\bigl(1 + (a + b - 1 - 1)\bigr) \times (a + b - 1 - 1) \\
    & = \frac{1}{2}(a + b - 1)(a + b - 2)
  \end{split}
\end{equation*}

So for an arbitrary element $(a, b) \in \mathbb{N} \times \mathbb{N}$, its
corresponding natural number k is:
\[ k = \frac{1}{2}(a + b - 1)(a + b - 2) + a \]

We can define the function $INDEX$, given $(a, b)$, to return the corresponding
natural number:
\[ INDEX((a, b)) = \frac{1}{2}(a + b - 1)(a + b - 2) + a \]

Then $INDEX$ is the bijection that the textbook proof talks about.
(\colorbox{red!100}{\textcolor{yellow}{TODO(ywen)}}: We still need to prove that
the function $INDEX$ is bijective.)

We can quickly verify $INDEX$ with the first few elements:

\begin{enumerate}
  \item $INDEX((1, 1)) = \frac{1}{2}(1 + 1 - 1)(1 + 1 - 2) + 1 = 1$
  \item $INDEX((1, 2)) = \frac{1}{2}(1 + 2 - 1)(1 + 2 - 2) + 1 = 2$
  \item $INDEX((2, 1)) = \frac{1}{2}(2 + 1 - 1)(2 + 1 - 2) + 2 = 3$
  \item $INDEX((1, 3)) = \frac{1}{2}(1 + 3 - 1)(1 + 3 - 2) + 1 = 4$
  \item $INDEX((2, 2)) = \frac{1}{2}(2 + 2 - 1)(2 + 2 - 2) + 2 = 5$
  \item $INDEX((3, 1)) = \frac{1}{2}(3 + 1 - 1)(3 + 1 - 2) + 3 = 6$
  \item etc.
\end{enumerate}

% ------------------------------
\subsubsection{Definition 0.3.33 (power set)}
% ------------------------------

The \textit{power set} of a set $A$, denoted by $\mathcal{P}(A)$, is the set of
all subsets of $A$.

For example, if $A := \{1, 2\}$, then $\mathcal{P}(A) = \{\emptyset, \{1\},
  \{2\}, \{1, 2\}\}$.

In general, for a finite set $A$ of cardinality $n$, the cardinality of
$\mathcal{P}(A)$ is $2^n$.

% ------------------------------
\subsubsection{Theorem 0.3.34 (Cantor)}
% ------------------------------

$|A| < |\mathcal{P}(A)|$. In particular, there exists no surjection from $A$
onto $\mathcal{P}(A)$.

\colorbox{lime!100}{\textbf{My notes}}: The textbook provides a proof but I
feel it omits some detailed steps for conciseness, so I'm trying to cover those
details in the notes here.

The first part of the proof, as quoted as follows, is quite easy to follow:

\begin{displayquote}
  There exists an injection $f: A \rightarrow \mathcal{P}(A)$. For any $x \in
    A$, define $f(x) := \{x\}$. Therefore, $|A| \leq |\mathcal{P}(A)|$.
\end{displayquote}

The second part needs a bit more explanation. The second part starts with the
following:

\begin{displayquote}
  To finish the proof, we must show that no function $g: A \rightarrow
    \mathcal{P}(A)$ is a surjection. Suppose $g: A \rightarrow \mathcal{P}(A)$ is
  a function. So for $x \in A$, $g(x)$ is a subset of $A$.
\end{displayquote}

Everything looks good so far. The proof then says:

\begin{displayquote}
  Define the set \[B:= \{x \in A: x \notin g(x)\}\].
\end{displayquote}

The question is: \textbf{Does such a set $B$ always exist?} In other words,
\textbf{is it guaranteed that $B$ is not empty?} To see why this question is
important, let's continue to see the subsequent part of the proof:

\begin{displayquote}
  Suppose for contradiction that there exists an $x_0$ such that $g(x_0) = B$.
  Either $x_0 \in B$ or $x_0 \notin B$. If $x_0 \in B$, then $x_0 \notin g(x_0)
    = B$, which is a contradiction. If $x_0 \notin B$, then $x_0 \in g(x_0) = B$,
  which is again a contradiction.
\end{displayquote}

Suppose that $B$ is an empty set. The sentence ``Suppose for contradiction that
there exists an $x_0$ such that $g(x_0) = B$'' still makes sense, because it is
perfectly fine that $g$ can be a function that maps $x_0$ to the empty set
$\emptyset$ which is a subset of $A$ (hence an element of $\mathcal{P}(A)$).
But then the conditional branch ``$x_0 \in B$'' wouldn't be satisfied because
$x_0$ can't belong to the empty set, so it makes no sense to talk about this
branch. So we are only left with the other conditional branch that ``$x_0 \notin
  B$'' (which is $\emptyset$ now). This condition always holds, but its conclusion
``$x_0 \in g(x_0) = B$'' makes no sense now because $B$ is $\emptyset$. We can
no longer solidly say the contradiction exists. Therefore, the validity of the
proof by contradiction is based on the fact that $B$ is always non-empty.

Is it? Yes, $B$ is always non-empty regardless how $g$ is defined. We can prove
this as follows:

\begin{itemize}
  \item Consider the case that $A$ is empty, so $\mathcal{P}(A):=
          \{\emptyset\}$. So the statement $|A| < |\mathcal{P}(A)|$ already holds for
        this case and we don't need to consider $B$ anymore.
  \item When $A$ is not empty, then $\emptyset \in \mathcal{P}(A)$. Now:
        \begin{itemize}
          \item If $\forall x \in A$ such that $g(x) \neq \emptyset$, that means
                $\emptyset$ is not in the range of $g$, so $g$ is not surjective.
          \item If $\exists x \in A$ such that $g(x) = \emptyset$, then we have
                $x \in A$ and $x \notin g(x) = \emptyset$, so $x \in B$, so in this case,
                $B$ is not empty. Now we can apply the proof by contradiction as shown in
                the textbook to prove that the general function $g$ can't be surjective.
        \end{itemize}
\end{itemize}

Now when we look back at the entire proof, it looks better that we prove the
statement $|A| < |\mathcal{P}(A)|$ by considering the following cases:
\begin{enumerate}
  \item When $A$ is $\emptyset$.
  \item When $A$ is not $\emptyset$, consider the two sub-cases:
        \begin{enumerate}
          \item $\forall x \in A$ such that $g(x) \neq \emptyset$.
          \item $\exists x \in A$ such that $g(x) = \emptyset$.
        \end{enumerate}
\end{enumerate}

% ------------------------------
\subsubsection{A few true statements (without proof)}
% ------------------------------

\begin{itemize}
  \item A set is infinite if and only if it is in one-to-one correspondence
        with a proper subset of itself.
  \item If $A \subset B$ and $B$ is countable, then $A$ is countable. If $A$ is
        uncountable, then $B$ is uncountable.
  \item If $|A| < |N|$, then $A$ is finite. Similarly, if $B$ is finite and $A
          \subset B$, then $A$ is finite.
  \item Uncountable sets do exist, e.g., $\mathcal{P}(\mathbb{N})$. This can be
        seen from the definition of uncountable, which is defined as ``not finite
        and not countably infinite'':
        \begin{enumerate}
          \item $\mathcal{P}(\mathbb{N})$ is surely not finite.
          \item $\mathcal{P}(\mathbb{N})$ is not countably infinite because
                $\mathcal{P}(\mathbb{N}) > |\mathbb{N}|$.
        \end{enumerate}
\end{itemize}

% =============================================================================
%
% Chapter 1: Real Numbers
%
% =============================================================================

\chapter{Real Numbers}

% =============================================================================
\section{Basic properties}
% =============================================================================

% ******************************
\subsection{Definition 1.1.1: Ordered set}
% ******************************

An \textbf{ordered set} is a set $S$, together with a relation $<$ such that:
\begin{enumerate}
  \item For any $x, y \in S$, \textbf{exactly one of} $x < y$, $x = y$, or $y < x$ holds.
  \item If $x < y$ and $y < z$, then $x < z$.
\end{enumerate}

We write $x \leq y$ if $x < y$ or $x = y$. We define $>$ and $\geq$ in the obvious way.

\colorbox{lime}{NOTE(ywen)}: $<$, $\le$, $>$, and $\ge$ are all binary relations (see \ref{def:binary-relation}). But
do they have all the three properties of reflexivity, symmetry, and transitivity?
\begin{itemize}
  \item $<$ is not reflexive (i.e., $x \nless x$), not symmetric (i.e., $x < y \nRightarrow y < x $). It is only
        transitive by definition above.
  \item $\le$ is reflexive (i.e., $x \le x$), not symmetric (i.e., $x \le y \nRightarrow y \le x$), transitive.
  \item $>$ is the same as $<$.
  \item $\ge$ is the same as $\le$.
\end{itemize}

% ******************************
\subsection{Definition 1.1.2: Bounds}
% ******************************

Let $E \subset S$ where $S$ is an ordered set.
\begin{enumerate}
  \item $\exists b \in S$, $\forall x \in E$. ($x \le b$): We say $E$ is \textbf{bounded above} and $b$ is an
        \textbf{upper bound} of $E$.
  \item $\exists b \in S$, $\forall x \in E$. ($x \ge b$): We say $E$ is \textbf{bounded below} and $b$ is a
        \textbf{lower bound} of $E$.
  \item $\exists b_0 \in S$. (IsUpperBound($b_0$, E, S) $\land$ ($\forall b$. (IsUpperBound(b, E, S) $\rightarrow$ $b_0 \le b$))):
        $b_0$ is called the \textbf{least upper bound} or the \textbf{supremum} of $E$. Denoted as $sup E := b_0$.
        (Suppose ``IsUpperBound(x, E, S)'' is a predicate that determines whether $x$ is an upper bound of $E$ in $S$.)
  \item $\exists b_0 \in S$. (IsLowerBound($b_0$, E, S) $\land$ ($\forall b$. (IsLowerBound(b, E, S) $\rightarrow$ $b_0 \ge b$))):
        $b_0$ is called the \textbf{greatest lower bound} or the \textbf{infimum} of $E$. Denoted as $inf E := b_0$.
        (Suppose ``IsLowerBound(x, E, S)'' is a predicate that determines whether $x$ is a lower bound of $E$ in $S$.)
\end{enumerate}

When $E$ is \textbf{both} bounded above and bounded below, we simply say that $E$ is \textbf{bounded}.

Supremum (or infimum) is \textbf{unique} (if it exists): If $b$ and $b_0$ are suprema of $E$:
\begin{itemize}
  \item Because $b$ is a supremum, it is also an upper bound, so the supremum $b_0$ is $\le b$ by definition.
  \item Because $b_0$ is a supremum, it is also an upper bound, so the supremum $b$ is $\le b_0$ by definition.
  \item The only way to satisfy $b_0 \le b$ and $b \le b_0$ is $b = b_0$, so $b = b_0$.
\end{itemize}

A supremum or infimum for $E$ (even if they exist) \textbf{need not be in} $E$. For example, the set $E = {x \in
  \mathbb{Q}: x < 1}$ has the least upper bound of 1 but $1 \notin E$.

% =============================================================================
%
% Chapter 2: Sequences and Series
%
% =============================================================================

\chapter{Sequences and Series}

TODO

% =============================================================================
%
% Chapter 3: Continuous Functions
%
% =============================================================================

\chapter{Continuous Functions}

TODO

% =============================================================================
%
% Chapter 4: The Derivative
%
% =============================================================================

\chapter{The Derivative}

TODO

% =============================================================================
%
% Chapter 5: The Riemann Integral
%
% =============================================================================

\chapter{The Riemann Integral}

TODO

% =============================================================================
%
% Chapter 6: Sequences of Functions
%
% =============================================================================

\chapter{Sequences of Functions}

TODO

% =============================================================================
%
% Chapter 7: Metric Spaces
%
% =============================================================================

\chapter{Metric Spaces}

TODO

% =============================================================================
%
% References
%
% =============================================================================

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{itemize}
  \item $[1]$ \href{https://ocw.mit.edu/courses/18-100a-real-analysis-fall-2020/resources/mit18_100af20_basic_analysis/}{Lebl, Jiří: \it{Basic Analysis I: Introduction to Real Analysis, Volume 1}}
\end{itemize}

\end{document}
