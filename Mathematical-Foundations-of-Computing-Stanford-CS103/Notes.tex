% Regarding `oneside` (https://stackoverflow.com/a/8371473/630364):
%
% `oneside` removes the blank pages between chapters.
% "Note that this method make the margins of all the pages the same. In
% `twoside`, the margins are different for the odd and the even pages".
\documentclass[12pt, letterpaper, oneside]{book}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{float}
\usepackage{hyperref}
\usepackage[letterpaper, textwidth=7.5in, textheight=8in]{geometry}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}
\usepackage{parskip}
\usepackage{pseudocode}
\usepackage{tikz}
\usetikzlibrary{arrows, automata, positioning, shapes.geometric}
\usepackage{titlesec}
\usepackage{xcolor}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}

\setcounter{secnumdepth}{4}

\title{
  Notes on \textit{Stanford: CS103: Mathematical Foundations of Computing}
}
\author{Yaobin Wen}
\date{July 2023 $\sim$ December 2023}

\begin{document}

\maketitle
\tableofcontents

\chapter*{Overview}
\addcontentsline{toc}{chapter}{Overview}

This document contains my study notes of the Stanford course \textit{CS103:
  Mathematical Foundations of Computing}. I use it for a few purposes:

\begin{enumerate}
  \item As a reference to quickly refresh my memory on the subjects.
  \item Keep the notes to help me understand the text that is not obvious for
        me to comprehend.
\end{enumerate}

% =============================================================================
%
% Set Theory
%
% =============================================================================

\chapter{Set Theory}

% =============================================================================
\section{Is 0 a natural number?}
% =============================================================================

Is $0$ a natural number? Does $\mathbb{N}$ include $0$?

There is no ``yes'' or ``no'' answer to these questions. Take a look at
\href{https://math.stackexchange.com/q/283/665777}{this question} and you will
find people all over the world have all kinds of understanding such as:
\begin{itemize}
  \item $0$ is definitely \textbf{NOT} a natural number and does not belong to
        $\mathbb{N}$.
  \item There are two conventions: In one convention, $0$ is not a natural
        number; in another convention, $0$ is a natural number.
  \item Natural numbers include 0; those positive integers are called ``whole
        numbers'':
        \begin{displayquote}
          I see plenty of both these days, but when I was at school and at
          university, I almost only saw them defined to be {0, 1, ..}. The elements
          of {1, 2, ..} were called the whole numbers in my school days.
        \end{displayquote}
  \item Natural numbers don't include 0; whole numbers, denoted as $\mathbb{W}$,
        include $0$.
  \item $\mathbb{N}$ includes 0; $\mathbb{N^+}$ is all the positive integers so
        it doesn't include 0.
  \item yadda yadda yadda...
\end{itemize}

So I don't think it makes sense to argue whether $0$ is a natural number or not.
We just need to define it clearly and move on.

In CS103, $\mathbb{N}$ represents the natural numbers that \textbf{include} $0$.
But if you read 18.100A, you'll see in that course, $\mathbb{N}$ does not
include $0$.

That's fine. The world is still in peace. So in this note, I will respect the
choice of CS103 and treat $\mathbb{N}$ as the set of all the non-negative
integers, i.e., \[\{0, 1, 2, 3, \ldots\}\].

% =============================================================================
\section{Introduction to Set Theory}
% =============================================================================

This first lecture introduces set theory. Because I have learned them in MIT
OCW 18.100A Real Analysis, I won't repeat the notes again because they can be
found in the notes for that course. Here, I will just mention a few points that
are not covered in 18.100A.

\begin{itemize}
  \item Set difference: 18.100A uses $A \setminus B$ to denote ``A minus B''.
        CS103 says $A - B$ is also used.
  \item \textbf{Symmetric difference}: $A \bigtriangleup B = (A \setminus B)
          \cup (B \setminus A) = (A \cup B) \setminus (A \cap B)$. For example, if
        $A = \{1, 2, 3\}$ and $B = \{3, 4, 5\}$, then $A \bigtriangleup B = \{1, 2,
          4, 5\}$. \colorbox{red}{\textcolor{yellow}{TODO:}} Add a Venn diagram.
\end{itemize}

% =============================================================================
\section{Cardinality}
% =============================================================================

% ******************************
\subsection{$|\mathbb{N}|$ and $|\mathbb{Z}|$}
% ******************************

I've already learned that $|\mathbb{Z}|$ and the set of all the even (or odd)
numbers have the same cardinality, but in CS103 I just learned $|\mathbb{N}| =
  |\mathbb{Z}|$. The bijection (see 18.100A) between $\mathbb{N}$ and $\mathbb{Z}$
is created in the following way:
\begin{itemize}
  \item All the non-negative even numbers (including $0$) in $\mathbb{N}$ are
        matched to one non-negative integers in $\mathbb{Z}$.
  \item All the non-negative odd numbers in $\mathbb{N}$ are matched to one
        negative integers in $\mathbb{Z}$.
\end{itemize}

Visualized, this can be shown as:
\begin{itemize}
  \item $0 \longleftrightarrow 0$
  \item $1 \longleftrightarrow -1$
  \item $2 \longleftrightarrow 1$
  \item $3 \longleftrightarrow -2$
  \item $4 \longleftrightarrow 2$
  \item $5 \longleftrightarrow -3$
  \item $6 \longleftrightarrow 3$
  \item $7 \longleftrightarrow -4$
  \item $8 \longleftrightarrow 4$
  \item etc.
\end{itemize}

In general, the bijection $f: \mathbb{N} \rightarrow \mathbb{Z}$ is defined as:

\[
  f(n) = \begin{cases}
    n / 2,        & if \ MOD(n) = 0 \\
    (-n - 1) / 2, & if \ MOD(n) = 1
  \end{cases}
\]

where $n \in \mathbb{N}$.

% ******************************
\subsection{Cantor's diagonalization proof}
% ******************************

\href{https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument}{Cantor's
  diagonalization proof} is used to prove $|S| < |\mathcal{P}(S)|$.

% ******************************
\subsection{$|Programs| < |Problems|$}
% ******************************

This is the most interesting conclusion I've seen in this lecture. The proof
process can be summarized as follows:
\begin{itemize}
  \item Because every valid computer program is essentially a string, but not
        every string is a valid program, we can see \[|Program| \leq |Strings|\].
  \item The problems in the world may or may not deal with sets of strings.
        Suppose $S$ denotes a set of strings, so one kind of problems is: Given a
        string $s$, determine whether $s \in S$. For example:
        \begin{itemize}
          \item Suppose S = \{ ``a'', ``b'', ``c'', $\ldots$, ``z'' \}, then the
                problem can be: Given a string $s$, determine whether $s$ is an English
                letter.
          \item Suppose S = \{ ``0'', ``1'', ``2'', ``3'', $\ldots$ \}, then the
                problem can be: Given a string $s$, determine whether $s$ represents a
                natural number.
        \end{itemize}
  \item Not to mention that other kinds of problems can exist and may not be
        converted to problems that deal with sets of strings.
  \item The conclusion is: $|Sets\ of\ strings| \leq |Problems|$. They could be
        equal, if all the problems can be converted to problems that deal with sets
        of strings.
  \item Therefore:
        \[|Program| \leq |Strings| < |Sets\ of\ strings| \leq |Problems|\]
\end{itemize}

% =============================================================================
%
% Mathematical Proofs
%
% =============================================================================

\chapter{Mathematical Proofs}

\begin{enumerate}
  \item $\blacksquare$ means ``end of proof''.
  \item Use the ``mugga mugga'' test to verify if the proof is written in valid
        sentences: Replace all the mathematical notation with ``mugga mugga''. What
        comes back should still be a valid sentence.
  \item The section ``Proofs as a Dialog'' is interesting: It says the variables
        used in a proof can be divided into three categories:
        \begin{enumerate}
          \item Proof writer picks. Example: ``Let r = n + 1''
          \item Proof reader picks. Example: ``Consider some $n \in \mathbb{N}$''
          \item Neither picks: The variable's value is determined by some laws or
                definition. Example: ``If $n$ is even, there exists a $k$ so that $n =
                  2k$''. In this case, if the reader chooses an arbitrary even number $n$,
                the corresponding $k$'s value is not chosen by either the reader or the
                writer but by the definition of ``even''.
        \end{enumerate}
        Knowing which variables are picked by whom can help verify if the proof
        makes sense. Don't change the variables you (the proof writer) don't own.
\end{enumerate}

% =============================================================================
%
% Indirect Proofs
%
% =============================================================================

\chapter{Indirect Proofs}

% =============================================================================
\section{Implication}
% =============================================================================

An implication can be denoted as $p \rightarrow q$:
\begin{enumerate}
  \item In mathematics, implication is directional.
  \item In mathematics, implications only say something about the consequent
        when the antecedent is true.
  \item In mathematics, implication says nothing about causality.
\end{enumerate}

\begin{tikzpicture}
  % Set Q, the enclosing set.
  \node [ellipse,
    draw=blue!60,
    fill=cyan!40,
    line width=1mm,
    minimum width=8cm,
    minimum height=5cm,
    label={-90:$Q(x)=True$}] (Q) at (0,0) {};

  % Set P, the enclosed set.
  \node [ellipse,
    draw=orange!80,
    fill=yellow!20,
    line width=1mm,
    minimum width=3cm,
    minimum height=2cm,
    label={$P(x)=True$}] (P) at (-1,0) {};

  % Element a where P(a) = True and Q(a) = True
  \node [
    regular polygon,
    draw,
    regular polygon sides=4,
    minimum size=0.5cm,
    fill=yellow] (a) at (-0.5,0) {a};
  \node[
    draw=black,
    left, black, fill=white] at (8,2) {P(a)=T and Q(a)=T};
  \draw [-stealth] (3.8,2) -- (0,0);

  % Element b where P(b) = True but Q(b) = False
  \node [
    regular polygon,
    draw,
    regular polygon sides=4,
    minimum size=0.5cm,
    fill=yellow] (b) at (2,0) {b};
  \node[
    draw=black,
    left, black, fill=white] at (9,0) {P(b)=T but Q(b)=F};
  \draw [-stealth] (4.9,0) -- (2.4,0);

  % Element c where P(c) = True and Q(c) = True
  \node [
    regular polygon,
    draw,
    regular polygon sides=4,
    minimum size=0.5cm,
    fill=yellow] (c) at (4,-2) {c};
  \node[
    draw=black,
    left, black, fill=white] at (10,-2) {P(c)=F and Q(c)=F};
  \draw [-stealth] (5.9,-2) -- (4.4,-2);
\end{tikzpicture}

% =============================================================================
\section{Negation}
% =============================================================================

The negation of the \textbf{universal} statement is the \textbf{existential}
statement:
\begin{enumerate}
  \item ``Every P is a Q'' $\rightarrow$ ``There is a P that is not a Q''
  \item ``$\forall x: P(x)=True$'' $\rightarrow$ ``$\exists x: P(x)=False$''
\end{enumerate}

The negation of the \textbf{existential} statement is the \textbf{universal}
statement:
\begin{enumerate}
  \item ``There exists a P that is a Q'' $\rightarrow$ ``Every P is not a Q''
  \item ``$\exists x: P(x)=True$'' $\rightarrow$ ``$\forall x: P(x)=False$''
\end{enumerate}

% -----------------------------------------------------------------------------
\subsection{Negate an implication}
% -----------------------------------------------------------------------------

How to negate the implication:

\begin{displayquote}
  If Nanni pays money to Ea-Nasir, then Ea-Nasir will give Nanni quality copper
  ingots.
\end{displayquote}

An implication should be viewed as in the form as follows:

\begin{displayquote}
  For any x, if P(x) is true, then Q(x) is also true.
\end{displayquote}

Therefore the implication above should be re-interpretted as follows:

\begin{displayquote}
  For any time when Nanni pays money to Ea-Nasir, then Ea-Nasir will give Nanni
  quality copper ingots.
\end{displayquote}

So the negation of the concerned implication is:

\begin{displayquote}
  There are times when Nanni pays money to Ea-Nasir, then Ea-Nasir does not
  give Nanni quality copper ingots.
\end{displayquote}

% =============================================================================
\section{Proof by Contrapositive}
% =============================================================================

The contrapositive of the implication

\begin{displayquote}
  If \textbf{\textcolor{teal}{P is true}}, then \textbf{\textcolor{violet}{Q is
      true}}
\end{displayquote}

is the implication

\begin{displayquote}
  If \textbf{\textcolor{violet}{Q is false}}, then \textbf{\textcolor{teal}{P
      is false}}.
\end{displayquote}

They are \textbf{equivalent}.

To prove the original proposition, one can prove its contrapositive, hence
``proof by contrapositive''.

% =============================================================================
\section{Biconditionals}
% =============================================================================

Biconditionals are the ``if and only if''s, namely $p \Leftrightarrow q$.

To prove a biconditional, we must prove two directions: $p \Rightarrow q$ and
$q \Rightarrow p$.

% =============================================================================
\section{Proof by contradiction}
% =============================================================================

Example of how to write a proof by contradiction:

\begin{displayquote}
  Theorem: There is no largest set.

  Proof: \textbf{\textcolor{violet}{Assume for the sake of contradiction}} that
  \textbf{\textcolor{orange}{there is a largest set; call it $S$}}.

  (Steps to show the contradiction.)

  (Conclusion) \textbf{\textcolor{teal}{We've reached a contradiction, so our
      assumption must have been wrong. Therefore, there is no largest set.}}
  $\blacksquare$
\end{displayquote}

% =============================================================================
%
% Propositional Logic
%
% =============================================================================

\chapter{Propositional Logic}

% =============================================================================
\section{Proposition}
% =============================================================================

% ******************************
\subsection{Definition}
% ******************************

A \textbf{proposition} is a statement that is, by itself, either \textbf{true}
or \textbf{false}.
\begin{itemize}
  \item Commands are not propositions. Example: Open the door.
  \item Questions are not propositions. Example: What day is it today?
\end{itemize}

A \textbf{propositional variable}, usually a lower-case English letter such as
$p$, $q$, $r$, represents a proposition.

A \textbf{propositional connective} expresses how propositions are related.
Some of the connectives are:
\begin{itemize}
  \item $\lnot p$: ``NOT p''; logical negation.
  \item $p \land q$: ``p AND q''; logical conjunction.
  \item $p \lor q$: ``p OR q''; logical disjunction (inclusive). ``inclusive''
        means $p$ and $q$ can be $true$ at the same time, while ``exclusive or''
        means $p$ and $q$ cannot be $true$ at the same time.
  \item $p \rightarrow q$: ``p implies q''; material condition.
  \item $p \leftrightarrow q$: ``p if and only if q'', which also means ``(p
        implies q) AND (q implies p)''.
  \item $\top$: ``(always) true'' ($\top$ looks like a upper-case ``T'' that
        can represent ``True''.)
  \item $\bot$: ``(always) false''
\end{itemize}

% ******************************
\subsection{Example of using $\top$ and $\bot$}
% ******************************

$\bot$ can be used to describe how proof by contradiction works. Suppose that
you want to prove $p$ is $true$ using proof by contradiction. The usual steps
are as follows:
\begin{enumerate}
  \item Assume $p$ is $false$.
  \item Derive a conclusion that is known as $false$ (e.g., ``3 is even'').
  \item Conclude that $p$ is $true$.
\end{enumerate}

Described in propositional logic, it is
\[
  (\lnot p \rightarrow \bot) \rightarrow p
\].

% ******************************
\subsection{Logical operator precedence}\label{logical_operator_precedence}
% ******************************

All the logical operators are \textbf{right-associative}.

In the order of highest to lowest, they are:
\begin{enumerate}
  \item $\lnot$
  \item $\land$
  \item $\lor$
  \item $\rightarrow$
  \item $\leftrightarrow$
\end{enumerate}

For example, the statement
\[
  \lnot x \rightarrow y \lor z \rightarrow x \lor y \and z
\]
can be grouped as follows:
\[
  (\lnot x) \rightarrow ((y \lor z) \rightarrow (x \lor (y \and z)))
\]

% ******************************
\subsection{Truth table}
% ******************************

I have already learned the truth tables for $\lnot$, $\land$, and $\lor$. What
is surprising to me is the truth table for $\rightarrow$:

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|ll}
    \cline{1-3}
    p & q & $p \rightarrow q$ &  & \\ [1ex] \cline{1-3}
    F & F & T                 &  & \\ [0.5ex] \cline{1-3}
    F & T & T                 &  & \\ [0.5ex] \cline{1-3}
    T & F & F                 &  & \\ [0.5ex] \cline{1-3}
    T & T & T                 &  & \\ [0.5ex] \cline{1-3}
  \end{tabular}
  \caption{Truth table for $p \rightarrow q$}
\end{table}

The first two lines are the most confusing to many people, and there are many
questions about them. Here are some of them:
\begin{itemize}
  \item \href{https://philosophy.stackexchange.com/q/26719/44172}{Shouldn't statements be considered equivalent based on their meaning rather than truth tables?}
  \item \href{https://philosophy.stackexchange.com/q/34082/44172}{Why are conditionals with false antecedents considered true?}
  \item \href{https://math.stackexchange.com/q/3098664/665777}{How Implication or Material/Concrete Conditional works when the antecedent is false and the consequent is true}
  \item \href{https://math.stackexchange.com/q/70736/665777}{In classical logic, why is ($p \rightarrow q$) True if p is False and q is True?}
  \item And many more...
\end{itemize}

I read some of the posts and then decided to stop because this looks like a
deep rabbit hole. For now, it is better to just accept the truth table and move
on. But to help understand them a little bit:
\begin{itemize}
  \item $p \rightarrow q \equiv \lnot p \lor q \equiv \lnot (p \land \lnot q)$.
  \item $\lnot (p \rightarrow q) \equiv p \land \lnot q$.
  \item It's helpful to think about ``Ex falso sequitur quodlibet'' which means
        ``from what is false any assertion validly follows''.
\end{itemize}

\colorbox{red}{\textcolor{yellow}{TODO:}} Figure out why $F \rightarrow F$ is
$T$ and $F \rightarrow T$ is $T$. Or figure out why $p \rightarrow q$ is
equivalent to $\lnot p \lor q$.

Here is the truth table for $p \leftrightarrow q$:

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|ll}
    \cline{1-3}
    p & q & $p \leftrightarrow q$ &  & \\ [1ex] \cline{1-3}
    F & F & T                     &  & \\ [0.5ex] \cline{1-3}
    F & T & F                     &  & \\ [0.5ex] \cline{1-3}
    T & F & F                     &  & \\ [0.5ex] \cline{1-3}
    T & T & T                     &  & \\ [0.5ex] \cline{1-3}
  \end{tabular}
  \caption{Truth table for $p \rightarrow q$}
\end{table}

% ------------------------------
\subsubsection{Vacuously true; trivially true}
% ------------------------------

An implication with a false antecedent is called \textbf{vacuously true}, such
as $F \rightarrow F: T$ and $F \rightarrow T: T$.

An implication with a true consequent is called \textbf{trivially true}, such
as $F \rightarrow T: T$ and $T \rightarrow T: T$.

Note that the implication $F \rightarrow F: T$ is \textbf{both vacuous and
  trivial}.

Note that the implication $T \rightarrow F: F$ is \textbf{neither of them}.

% ******************************
\subsection{de Morgan's Laws}
% ******************************

\begin{itemize}
  \item $\lnot (p \land q) \equiv \lnot p \lor \lnot q$
  \item $\lnot (p \lor q) \equiv \lnot p \land \lnot q$
\end{itemize}

% =============================================================================
%
% First-Order Logic
%
% =============================================================================

\chapter{First-Order Logic}

% =============================================================================
\section{What is First-Order Logic?}
% =============================================================================

First-order logic:
\begin{enumerate}
  \item It is a a logical system for reasoning about properties of objects.
        There are two kinds of objects:
        \begin{enumerate}
          \item \textbf{Constants}: A symbol that refers to a specific object. For
                example, ``You'', ``Me'', ``EastAtlanta'', ``137''. Note that
                numbers such as ``137'' are \textbf{not} built in to first-order logic.
                They are just constant symbols like ``You'' and ``Me''.
          \item \textbf{Variables}: A symbol that serves as a placeholder to
                represent any object in a particular set. Usually this is used when
                \textbf{quantifiers} is used (because when quantifiers are used, we
                don't specify just one specific object, so we need a symbol to
                represent any of such objects we are referring to).
        \end{enumerate}
  \item It is built upon propositional logic.
  \item It uses the following three devices to describe more complex logic:
        \begin{enumerate}
          \item \textbf{predicates}: Describe \textbf{properties} of objects. It
                can take one or more objects as input and turn them into a proposition
                which is evaluated as $true$ or $false$.
                \begin{enumerate}
                  \item Binary predicates are sometimes written in \textbf{infix
                          notation}. For example, instead of writing ``$<(x, 8)$'', it's
                        more natural to write ``$x < 8$''.
                \end{enumerate}
          \item \textbf{functions}: Map objects to objects.
          \item \textbf{quantifiers}: Describe the quantities of objects, mainly
                two cases: ``for all objects'' and ``for some objects''.
        \end{enumerate}
  \item Examples:
        \begin{enumerate}
          \item $Likes(You, Eggs) \land Likes(You, Tomato) \rightarrow
                  Likes(You, Shakshuka)$
          \item $In(MyHeart, Havana) \land TookBackTo(Him, Me, EastAtlanta)$
        \end{enumerate}
\end{enumerate}

% =============================================================================
\section{Equality}
% =============================================================================

\begin{enumerate}
  \item The predicates ``$=$'' and ``$\neq$'' indicate whether two
        \textbf{objects} are equal or not.
  \item ``$\leftrightarrow $'' indicates that two \textbf{propositions} are
        equal. Note that ``$\leftrightarrow $'' is not a predicate because
        predicates are applied to objects to describe the objects' properties, but
        ``$\leftrightarrow $'' is applied to propositions to describe their logical
        relations.
\end{enumerate}

% =============================================================================
\section{Type-Checking Table}
% =============================================================================

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    \textbf{}            & \textbf{...operate on..} & \textbf{...produce} \\ [1ex] \hline
    \textbf{Connectives} & propositions             & a proposition       \\ [1ex] \hline
    \textbf{Predicates}  & objects                  & a proposition       \\ [1ex] \hline
    \textbf{Functions}   & objects                  & an object           \\ [1ex] \hline
  \end{tabular}
  \caption{Type-Checking Table}
  \label{FOL_type_checking_table}
\end{table}

% =============================================================================
\section{Existential and Universal Quantifiers}
% =============================================================================

% ******************************
\subsection{General form}
% ******************************

The general form of using the quantifiers is as follows:
\begin{enumerate}
  \item $\exists x. PROPERTY(x)$: Some $x$ has the property ``PROPERTY''.
  \item $\forall x. PROPERTY(x)$: All $x$ has the property ``PROPERTY''.
\end{enumerate}

To describe the set that $x$ belongs to, use the following forms:
\begin{enumerate}
  \item $\exists x. (x \in X \rightarrow PROPERTY(x))$
  \item $\forall x. (x \in X \rightarrow PROPERTY(x))$
\end{enumerate}

For example, ``For any natural number $n$, $n$ is even if and only if $n^2$ is
even'' can be translated into
\[
  \forall n. (n \in \mathbb{N} \rightarrow (Even(n) \leftrightarrow Even(n^2)))
\]

% ******************************
\subsection{Edge cases}
% ******************************

I'm familiar with the existential quantifier ``$\exists$'' and the universal
quantifier ``$\forall$'', but I didn't think about the two edge cases before:
\begin{enumerate}
  \item $\exists x. (x \in \emptyset \rightarrow PROPERTY(x) = \bot)$, i.e.,
        existentially-quantified statements are always \textbf{false} in an empty
        world because \textbf{nothing exists}.
  \item $\forall x. (x \in \emptyset \rightarrow PROPERTY(x) = \top)$, i.e.,
        universally-quantified statements are said to be \textbf{vacuously true} in
        an empty world.
\end{enumerate}

% ******************************
\subsection{Association}
% ******************************

The variable is scoped just to the statement being quantified. For example:
\[(\exists x. BIGGER(A, x)) \land (\exists y. SMALLER(y, B))\]

\begin{enumerate}
  \item $x$ is only applicable to ``BIGGER''.
  \item $y$ is only applicable to ``SMALLER''.
\end{enumerate}

% ******************************
\subsection{Precedence}
% ******************************

Quantifiers have precedence just below ``$\lnot$''. See
\ref{logical_operator_precedence}.

Therefore, the statement \[\exists x. P(x) \land R(x) \land Q(x)\] is parsed as
\[(\exists x. P(x)) \land R(x) \land Q(x)\] which is \textbf{syntactically
  invalid} because the variable $x$ is out of scope for $R$ and $Q$.



% =============================================================================
%
% Functions
%
% =============================================================================

\chapter{Functions}

 (TODO)

% =============================================================================
%
% Graphs
%
% =============================================================================

\chapter{Graphs}

 (TODO)

% =============================================================================
%
% Mathematical Induction
%
% =============================================================================

\chapter{Mathematical Induction}

 (TODO)

% =============================================================================
%
% Finite Automata
%
% =============================================================================

\chapter{Finite Automata}

% =============================================================================
\section{Computability Theory}
% =============================================================================

\textbf{Two key questions}: How can we prove what computers can and can't do...
\begin{enumerate}
  \item ... so that our results are still true in 20 years?
  \item ... without multi-hundred page proofs?
\end{enumerate}

% =============================================================================
\section{Finite Automata: Modeling Finite Computation}
% =============================================================================

\colorbox{lime}{\textbf{NOTE(ywen)}}: The automata seem to be different from the ``state machines'' I have understood
so far. The ``state machines'' I've been using are those whose every state is an ``accepting state'' (see below for the
meaning of an ``accepting state''), but an automaton may have accepting states (which the automaton returns ``Yes'' to
indicate the acceptance) and rejecting states (which the automaton returns ``No'' to indicate the rejection). In other
words, the ``state machines'' I've been using all the time are a subset of automata.

Slide 42 says:
\begin{displayquote}
  As a simplifying assumption, we'll assume that we just need to get a single
  bit of output. That is, our machines will just say \textbf{YES} or
  \textbf{NO}.
\end{displayquote}

\colorbox{red}{\textcolor{yellow}{TODO:}} I need to learn how the generalization happens.

Slide 44 introduces the following terms:
\begin{itemize}
  \item \textbf{accepting states}: \colorbox{red}{\textcolor{yellow}{TODO:}}
        How to determine if a state is an accepting state?
  \item \textbf{accepts}: If the device ends in an \textbf{accepting state}
        after seeing all the input, it \textbf{accepts} the input (says
        \textbf{YES}).
  \item \textbf{rejects}: If the device does not end in an \textbf{accepting
          state} after seeing all the input, it \textbf{rejects} the input (says
        \textbf{NO}).
\end{itemize}

Finite automata model computers where:
\begin{enumerate}
  \item memory is finite.
  \item the computation produces as YES/NO answer. (``YES/NO" is similar to
        ``True/False". In other words, given the input, an automaton outputs
        ``True/False", so an automaton can be viewed as a \textbf{predicate}. See
        ``first-order logic'' for the meaning of a ``predicate''.)
\end{enumerate}

% ******************************
\subsection{Finite Automata and Languages}
% ******************************

In order to define finite automata and their languages, we need to define ``languages'' first.

% ------------------------------
\subsubsection{Alphabet}
% ------------------------------

An \textbf{alphabet} is:
\begin{enumerate}
  \item a \textbf{finite}, \textbf{nonempty} set of symbols called \textbf{characters}.
  \item denoted by $\Sigma$.
\end{enumerate}

For example, $\Sigma = \{a, b\}$ is an alphabet over the characters ``a'' and ``b''.

% ------------------------------
\subsubsection{Strings}
% ------------------------------

A \textbf{string} over an alphabet $\Sigma$ is a \textbf{finite} sequence of characters drawn from $\Sigma$. The
\textbf{empty string} has no characters and is denoted $\epsilon$. The set of \textbf{all strings} composed from the
characters in $\Sigma$ is denoted $\Sigma^*$.

For example, some strings over $\Sigma = \{a, b\}$ are: ``a'', ``ababbabbbababba'', ``abbabbab''.

\colorbox{lime}{\textbf{NOTE(ywen)}}: A tip to remember the symbols: In regular expressions in computer science, we use
asterisks (``*'') to denote the quantifier ``repeating the previous item zero or more times''. Therefore, $\Sigma^*$
can be seen as ``repeating the characters in the set of $\Sigma$ zero or more times.'' When it's zero times, the
resulting string is the empty string $\epsilon$; when it's one or more times, the resulting string is a non-empty one.

% ------------------------------
\subsubsection{Languages}
% ------------------------------

A \textbf{language}:
\begin{enumerate}
  \item is a set of \textbf{strings}.
  \item is a language over $\Sigma$ if it is a subset of $\Sigma^*$.
\end{enumerate}

% ------------------------------
\subsubsection{Finite Automata and Languages} \label{automata:automata-and-languages}
% ------------------------------

Let $A$ be an automaton that processes strings drawn from an alphabet $\Sigma$. The language of $A$, denoted
$\mathfrak{L}(A)$, is the set of strings over $\Sigma$ that $A$ accepts:

\[
  \mathfrak{L}(A) = \{ w \in \Sigma^* | A \ accepts \ w \}
\]

Here ``$A$ accepts $w$'' is the general form of the rule about $w$. In a particular example of automaton, it can be
``$w$ ends in the character $a$''.

% ------------------------------
\subsubsection{Summary}
% ------------------------------

\begin{enumerate}
  \item A \textbf{finite automaton} is a collection of \textbf{states} joined by \textbf{transitions}.
  \item Some state is designated as the \textbf{start state}.
  \item Some number of states are designated as \textbf{accepting states}. These accepting states make the automaton
        return ``Yes''.
  \item The automaton processes a string by beginning in the start state and following the indicated transitions.
  \item If the automaton ends in an accepting state, it \textbf{accepts} the input. Otherwise, the automaton
        \textbf{rejects} the input.
  \item The \textbf{language} of an automaton is the set of strings it accepts.
\end{enumerate}

% =============================================================================
\section{Deterministic Finite Automaton (DFA)}
% =============================================================================

% ******************************
\subsection{Two problems of indeterminism}
% ******************************

First problem: \textbf{No transition is defined out of a state on some input.} For example, in the following automaton,
the entire possible input space is $\{0, 1\}$. However, for the states $q_0$ and $q_2$, the transitions on the input
$1$ are \textbf{undefined}; for the state $q_1$, the transition on the input $0$ is \textbf{undefined}.

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                      {$q_0$};
  \node[state,accepting] (q_1) [below right of=q_0] {$q_1$};
  \node[state]           (q_2) [below left of=q_0]  {$q_2$};

  \path[->]
  (q_0) edge node {0} (q_1)
  (q_1) edge node {1} (q_2)
  (q_2) edge node {0} (q_0)
  ;
\end{tikzpicture}

Second problem: \textbf{There are multiple transitions out of a state on some input.} For example, in the following
automaton, on the input $0$, the state $q_1$ has two possible out transitions.

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                 {$q_0$};
  \node[state]           (q_1) [right of=q_0]  {$q_1$};
  \node[state,accepting] (q_2) [below of=q_1]  {$q_2$};

  \path[->]
  (q_0) edge              node {0,1} (q_1)
  (q_1) edge [loop right] node {0,1} (q_1)
  (q_1) edge              node {0}   (q_2)
  (q_2) edge [bend left]  node {0,1} (q_0)
  ;
\end{tikzpicture}

Therefore, in order to reason about the limits of what finite automata can and cannot do, we need to formally specify
their behavior in \textbf{all cases}. All of the following need to be \textbf{defined} or \textbf{disallowed}:
\begin{itemize}
  \item What happens if there is no transition out of a state on some input?
  \item What happens if there are multiple transitions out of a state on some input?
\end{itemize}

% ******************************
\subsection{DFAs}
% ******************************

\begin{itemize}
  \item A DFA is defined relative to some alphabet $\Sigma$.
  \item For each state in the DFA, there must be \textbf{exactly one} transition defined for each symbol in $\Sigma$.
  \item There is \textbf{exactly one} start state.
  \item There are \textbf{zero or more} accepting states.
\end{itemize}

% ------------------------------
\subsubsection{The DFA for valid C-style comments}
% ------------------------------

The problem to solve is to design a DFA that represents any valid C-style comments. We use the symbol ``a'' to be the
placeholder for the characters that are not an asterisk or slash.

Note that the following are valid C-style comments:
\begin{itemize}
  \item /*/*/
  \item /*//*/
  \item /*/***/
\end{itemize}

In my first attempt, I designed the following DFA. Note this design has a few mistakes. See my notes below.

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                 {$q_0$};
  \node[state]           (q_1) [right of=q_0]  {$q_1$};
  \node[state]           (q_2) [right of=q_1]  {$q_2$};
  \node[state]           (q_3) [right of=q_2]  {$q_3$};
  \node[state]           (q_4) [below of=q_2]  {$q_4$};
  \node[state,accepting] (q_5) [below of=q_4]  {$q_5$};
  \node[state]           (q_6) [right of=q_5]  {$q_6$};

  \path[->]
  (q_0) edge                    node {/}        (q_1)
  (q_0) edge [loop below]       node {a, *}     ()
  (q_1) edge                    node {*}        (q_2)
  (q_1) edge [loop below]       node {a, /}     ()
  (q_2) edge                    node {a, /}     (q_3)
  (q_2) edge                    node {*}        (q_4)
  (q_3) edge [loop right]       node {a, /}     ()
  (q_3) edge [swap]             node {*}        (q_4)
  (q_4) edge [bend right, swap] node {a}        (q_3)
  (q_4) edge [loop left]        node {*}        ()
  (q_4) edge                    node {/}        (q_5)
  (q_5) edge                    node {$\Sigma$} (q_6)
  (q_6) edge [loop above]       node {$\Sigma$} ()
  ;
\end{tikzpicture}

Unfortunately, there are a few errors in this design:
\begin{enumerate}
  \item On the state $q_0$, the edge \{a, *\} that goes into itself is wrong because it means if a ``/'' follows a
        bunch of ``a'' and ``*'', the result can still be a valid C-style comment. For example, ``a/a/aa/*aa*/'' is
        not a valid C-style comment, but my DFA would consider it valid, which is an error. \textbf{The lesson:} When
        validating the design, we need to use three \textbf{types} of input:
        \begin{enumerate}
          \item \textbf{Full valid} input. For example, ``/* aaa */'' is a full valid input.
          \item \textbf{Full invalid} input. For example, ``*//**'' is a full invalid input.
          \item \textbf{Partial invalid (valid prefix + invalid suffix)} input. For example, ``/* a */*/**''. The
                former part ``/* a */'' is a valid input but the latter part ``*/**'' is an invalid input.
          \item \textbf{Partial invalid (invalid prefix + valid suffix)} input. For example, ``*/**/* a */''. The
                former part ``*/**'' is an invalid input but the latter part ``/* a */'' is a valid input.
        \end{enumerate}
  \item Likewise, on the state $q_1$, the edge \{a, /\} that goes into itself is wrong for the similar reason. For
        example, ``/a/aaa*a*/'' is not a valid C-style comment, but my DFA would consider it valid.
  \item The states $q_2$ and $q_3$ are equivalent and can be merged as one state. One way to check if two states are
        \textbf{possibly} redundant can be checking whether the two states have the same out-transitions. In my DFA,
        the state $q_2$ has two out-transitions: the edge \{a, /\} to $q_3$ and the edge \{*\} to $q_4$. The state
        $q_3$ has the same two out-transitions. This means $q_2$ and $q_3$ may be equivalent and should be examined
        more closely.
\end{enumerate}

The correct DFA should be the following one (which is equivalent to the one in the course slides):

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                 {$q_0$};
  \node[state]           (q_1) [right of=q_0]  {$q_1$};
  \node[state]           (q_2) [right of=q_1]  {$q_2$};
  \node[state]           (q_3) [below of=q_2]  {$q_3$};
  \node[state,accepting] (q_4) [below of=q_3]  {$q_4$};
  \node[state]           (q_5) [left of=q_4]   {$q_5$};

  \path[->]
  (q_0) edge                    node {/}        (q_1)
  (q_0) edge [bend right, swap] node {a, *}     (q_5)
  (q_1) edge                    node {*}        (q_2)
  (q_1) edge                    node {a, /}     (q_5)
  (q_2) edge [loop right]       node {a, /}     ()
  (q_2) edge [bend right, swap] node {*}        (q_3)
  (q_3) edge [bend right, swap] node {a}        (q_2)
  (q_3) edge [loop right]       node {*}        ()
  (q_3) edge                    node {/}        (q_4)
  (q_4) edge                    node {$\Sigma$} (q_5)
  (q_5) edge [loop below]       node {$\Sigma$} ()
  ;
\end{tikzpicture}

% =============================================================================
\section{NFAs}
% =============================================================================

% ******************************
\subsection{Definition and properties} \label{automata:nfa:definition-and-properties}
% ******************************

An \textbf{NFA} is a \textbf{N}ondeterministic \textbf{F}inite \textbf{A}utomaton.

An NFA has a \textbf{finite number} (0, 1, or more) of transitions available to make at each state. In contrast, a DFA
has \textbf{exactly one} transition at each state. The machine \textbf{accepts} (the input) if \textbf{any} series of
transitions leads to an accepting state. The following NFA shows two cases of \textbf{indeterminism}:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                 {$q_0$};
  \node[state]           (q_1) [right of=q_0]  {$q_1$};
  \node[state,accepting] (q_2) [right of=q_1]  {$q_2$};

  \path[->]
  (q_0) edge                    node {1}        (q_1)
  (q_0) edge [loop below]       node {0,1}      ()
  (q_1) edge                    node {1}        (q_2)
  ;
\end{tikzpicture}

\begin{enumerate}
  \item The state $q_0$ has \textbf{more than one} possible transitions for the input $1$: it could go to state $q_1$,
        or it could go back to itself.
  \item The state $q_1$ has \textbf{zero} transition for the input $0$: the input $0$ makes no sense in the context of
        $q_1$. When we are in the state $q_1$ but then the input is $0$, we say the automaton \textbf{dies} and this
        particular series of choices do not accept.
\end{enumerate}

\colorbox{lime}{\textbf{NOTE(ywen)}}: Understanding ``any series of transitions'': Given the NFA above, consider the
input ``$1 \rightarrow 0 \rightarrow 1 \rightarrow 1$''. There are a few ways to go through the NFA:

\begin{enumerate}
  \item $q_0 \rightarrow q_0 \rightarrow q_0 \rightarrow q_0 \rightarrow q_0$: reject
  \item $q_0 \rightarrow q_0 \rightarrow q_0 \rightarrow q_0 \rightarrow q_1$: reject
  \item $q_0 \rightarrow q_0 \rightarrow q_0 \rightarrow q_1 \rightarrow q_2$: \textbf{accept}
  \item $q_0 \rightarrow q_1 $: die (i.e., reject)
\end{enumerate}

Each one is a series of transitions to go through the NFA. ``any series of transitions'' means if any one of the
possible series of transitions can lead to acceptance (i.e., the 3rd series in the example above), the input is
considered accepted.

% ******************************
\subsection{The \texorpdfstring{$\epsilon$}{}-Transitions}
% ******************************

The $\epsilon$-transitions are \textbf{NFA-specific}. An NFA may follow \textbf{any number} of $\epsilon$-transitions
at \textbf{any time} without consuming \textbf{any input}. NFAs are \textbf{not required} to follow $\epsilon$-
transitions. It's simply another option at the machine's disposal.

For example, given the following NFA,

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                 {$q_0$};
  \node[state]           (q_1) [right of=q_0]  {$q_1$};
  \node[state]           (q_2) [right of=q_1]  {$q_2$};
  \node[state]           (q_3) [below of=q_0]  {$q_3$};
  \node[state,accepting] (q_4) [right of=q_3]  {$q_4$};
  \node[state]           (q_5) [right of=q_4]  {$q_5$};

  \path[->]
  (q_0) edge                    node {a}             (q_1)
  (q_0) edge [swap]             node {$\epsilon$}    (q_3)
  (q_1) edge                    node {a}             (q_2)
  (q_2) edge [bend right, swap] node {a}             (q_0)
  (q_3) edge                    node {b, $\epsilon$} (q_4)
  (q_4) edge                    node {$\epsilon$}    (q_1)
  (q_4) edge                    node {b}             (q_5)
  (q_5) edge [bend left]        node {b}             (q_3)
  ;
\end{tikzpicture}

consider the series of choices ``$b \rightarrow a \rightarrow a \rightarrow b \rightarrow b$''. There is no transition
on the input $b$ in the starting state $q_0$. However, because $q_0$ has an $\epsilon$-transition, before the first
input choice $b$ is consumed, the automaton can transition from $q_0$ to $q_3$ which has a transition on $b$. Likewise,
$q_4$ doesn't have any transition for $a$, but it has an $\epsilon$-transition to $q_1$ which has a transition on $a$.

The input series of choices ``$b \rightarrow a \rightarrow a \rightarrow b \rightarrow b$'' lead to the accepting state
by going through the states in the following sequence from $q_0$:

\[
  \epsilon \rightarrow b \rightarrow \epsilon \rightarrow a \rightarrow a \rightarrow \epsilon \rightarrow \epsilon
  \rightarrow b \rightarrow b \rightarrow \epsilon
\]

Note that the $\epsilon$-transitions were applied at the very beginning and at the very end. This is what ``\textbf{any
  time}'' means.

% ******************************
\subsection{NFA: Perfect positive guessing} \label{automata:nfa:perfect-positive-guessing}
% ******************************

Perfect positive guessing means an NFA has the \textbf{magical superpower} that enables them to guess choices that lead
to an accepting state. (If there are no such choices, the NFA guesses any one of the wrong guesses.)

For example, given the following NFA,

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                 {$q_0$};
  \node[state]           (q_1) [right of=q_0]  {$q_1$};
  \node[state]           (q_2) [right of=q_1]  {$q_2$};
  \node[state,accepting] (q_3) [right of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge                    node {a}        (q_1)
  (q_0) edge [loop above]       node {$\Sigma$} ()
  (q_1) edge                    node {b}        (q_2)
  (q_2) edge                    node {a}        (q_3)
  ;
\end{tikzpicture}

consider the input ``$a \rightarrow b \rightarrow a \rightarrow b \rightarrow a$''. To land on the accepting state
$q_3$, the right transitions to go are $q_0 \rightarrow q_0 \rightarrow q_0 \rightarrow q_1 \rightarrow q_2 \rightarrow
  q_3$. The ``magical superpower'' means the NFA can magically figures out these transitions to choose them to arrive
at the accepting state.

This magical superpower has something to do with the property of NFAs that ``the machine \textbf{accepts} (the input)
if \textbf{any} series of transitions leads to an accepting state.'' See \ref{automata:nfa:definition-and-properties}.

However, as the course slides said:

\begin{displayquote}
  There is no known way to physically model this intuition of nondeterminism - this is quite a departure from reality!
\end{displayquote}

% ******************************
\subsection{NFA: Massive parallelism}
% ******************************

An NFA can be thought of as a DFA that can be \textbf{in many states at once}. At each point in time, when the NFA
needs to follow a transition, it tries \textbf{all the options at the same time}.

Use the same NFA as in the section \ref{automata:nfa:perfect-positive-guessing} for example. Consider the same series
of transitions ``$a \rightarrow b \rightarrow a \rightarrow b \rightarrow a$''. Initially, the NFA starts at $q_0$. For
the first transition ``a'', it can move to two states: going through the $\Sigma$ transition back to itself; going
through the $a$ transition to $q_1$. Massive parallelism says the NFA can do these two transitions at the same time,
ending up in both states (\colorbox{yellow}{yellow} and \colorbox{green}{green}):

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,fill=yellow]   (q_0)                 {$q_0$};
  \node[state,fill=green]            (q_1) [right of=q_0]  {$q_1$};
  \node[state]                       (q_2) [right of=q_1]  {$q_2$};
  \node[state,accepting]             (q_3) [right of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge                    node {a}        (q_1)
  (q_0) edge [loop above]       node {$\Sigma$} ()
  (q_1) edge                    node {b}        (q_2)
  (q_2) edge                    node {a}        (q_3)
  ;
\end{tikzpicture}

The next transition is ``b''. Because now the NFA is at both $q_0$ and $q_1$, both of which can do the transition $b$,
so the NFA can do both again. $q_0$ still moves to itself; $q_1$ moves to $q_2$. See the following result.

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,fill=yellow]   (q_0)                 {$q_0$};
  \node[state]                       (q_1) [right of=q_0]  {$q_1$};
  \node[state,fill=green]            (q_2) [right of=q_1]  {$q_2$};
  \node[state,accepting]             (q_3) [right of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge                    node {a}        (q_1)
  (q_0) edge [loop above]       node {$\Sigma$} ()
  (q_1) edge                    node {b}        (q_2)
  (q_2) edge                    node {a}        (q_3)
  ;
\end{tikzpicture}

The next transition is ``a'' again. Now there are three possible transitions to do: $q_0 \rightarrow q_0$,
$q_0 \rightarrow q_1$, $q_2 \rightarrow q_3$. Massive parallelism says they can all happen at the same time, resulting
in the new states below:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,fill=cyan]     (q_0)                 {$q_0$};
  \node[state,fill=yellow]           (q_1) [right of=q_0]  {$q_1$};
  \node[state]                       (q_2) [right of=q_1]  {$q_2$};
  \node[state,accepting,fill=green]  (q_3) [right of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge                    node {a}        (q_1)
  (q_0) edge [loop above]       node {$\Sigma$} ()
  (q_1) edge                    node {b}        (q_2)
  (q_2) edge                    node {a}        (q_3)
  ;
\end{tikzpicture}

The next transition is ``b''. The state $q_3$ doesn't have a transition defined for ``b'', so we say the NFA
\textbf{dies} for this particular choice of transitions. But the transition ``b'' still works for $q_0$ and $q_1$. So
the NFA's new state will be as follows:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,fill=cyan]     (q_0)                 {$q_0$};
  \node[state]                       (q_1) [right of=q_0]  {$q_1$};
  \node[state,fill=yellow]           (q_2) [right of=q_1]  {$q_2$};
  \node[state,accepting]             (q_3) [right of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge                    node {a}        (q_1)
  (q_0) edge [loop above]       node {$\Sigma$} ()
  (q_1) edge                    node {b}        (q_2)
  (q_2) edge                    node {a}        (q_3)
  ;
\end{tikzpicture}

The last transition is ``a''. At this moment, the NFA is at the state $q_0$ and $q_2$. Its previous state $q_3$ had
``died''. The new state will be as follows:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,fill=magenta]  (q_0)                 {$q_0$};
  \node[state,fill=cyan]             (q_1) [right of=q_0]  {$q_1$};
  \node[state]                       (q_2) [right of=q_1]  {$q_2$};
  \node[state,accepting,fill=yellow] (q_3) [right of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge                    node {a}        (q_1)
  (q_0) edge [loop above]       node {$\Sigma$} ()
  (q_1) edge                    node {b}        (q_2)
  (q_2) edge                    node {a}        (q_3)
  ;
\end{tikzpicture}

Now we've finished running all the transitions. We are in at least one accepting state (the \colorbox{yellow}{$q_3$}),
so there is some path that gets us to an accepting state.

In contrast, the input sequence ``$a \rightarrow b \rightarrow a \rightarrow b$'' will not lead to any accepting state.
See the course slides for the detailed state changes.

% ******************************
\subsection{Designing NFAs}
% ******************************

A good model for designing NFAs: \textbf{guess-and-check}:
\begin{itemize}
  \item Is there some information that you'd really like to have? Have the machine \textbf{nondeterministically guess}
        that information.
  \item Then, have the machine \textbf{deterministically check} that the choice was correct.
\end{itemize}

\colorbox{lime}{\textbf{NOTE(ywen)}}: According to my own experience, given a problem:
\begin{itemize}
  \item It seems to be helpful to \textbf{directly} describe the \textbf{desired ending states}, and then figure out
        where the nondeterminism can fit in.
  \item Remember NFAs have two special devices:
        \begin{itemize}
          \item The $\epsilon$-transitions.
          \item An NFA can die to indicate the input is rejected.
        \end{itemize}
\end{itemize}

The course slides provide two examples. The second example demonstrates the use of the two devices: Design an automaton
that recognizes the language $L = \{ w \in \{a, b, c\}* | at \ least \ one \ of \ a, b, or \ c \ is \ not \ in \ w \}$.
The DFA solution is complicated (see the course slides), but the NFA solution is more simple:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]     (q_0)                 {$q_0$};
  \node[state,accepting]   (q_2) [right of=q_0]  {$q_2$};
  \node[state,accepting]   (q_1) [above of=q_2]  {$q_1$};
  \node[state,accepting]   (q_3) [below of=q_2]  {$q_3$};

  \path[->]
  (q_0) edge [bend left]  node {$\epsilon$}      (q_1)
  (q_1) edge [loop right] node {a,b}             ()
  (q_0) edge              node {$\epsilon$}      (q_2)
  (q_2) edge [loop right] node {a,c}             ()
  (q_0) edge [bend right] node {$\epsilon$}      (q_3)
  (q_3) edge [loop right] node {b,c}             ()
  ;
\end{tikzpicture}

If the input sequence doesn't contain ``a'', the solution relies on the NFA be able to magically pick the
$\epsilon$-transition to $q_3$; if the input sequence doesn't contain ``b'', the solution relies on the NFA be able to
magically pick the $\epsilon$-transition to $q_2$; if the input sequence doesn't contain ``c'', the solution relies on
the NFA be able to magically pick the $\epsilon$-transition to $q_1$.

If the input sequence contains all the letters ``a'', ``b'', and ``c'', it doesn't matter which transition to take at
$q_0$. The NFA will always die at a point, which indicates the rejection of the input.

% =============================================================================
\section{Describe DFAs and NFAs as tables}
% =============================================================================

DFAs and NFAs are essentially graphs, so we can use adjacent matrices (with some modifications) to describe them.

% ******************************
\subsection{Tabular DFAs}
% ******************************

Because a DFA is deterministic, the state of the DFA after a transition can be completely described by a state in the
DFA, so the transitions are truly from one state to another state.

For example, given the following DFA:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,accepting] (q_0) {$q_0$};
  \node[state]                   (q_1) [right of=q_0] {$q_1$};
  \node[state]                   (q_2) [right of=q_1]{$q_2$};
  \node[state,accepting]         (q_3) [right of=q_2] {$q_3$};

  \path[->]
  (q_0) edge [loop above] node {1}        ()
  (q_0) edge              node {0}        (q_1)
  (q_1) edge              node {1}        (q_2)
  (q_1) edge [bend left]  node {0}        (q_3)
  (q_2) edge              node {0}        (q_3)
  (q_2) edge [bend left]  node {1}        (q_0)
  (q_3) edge [loop right] node {$\Sigma$} (q_3)
  ;
\end{tikzpicture}

We can use the following modified adjacent matrix to describe it with two modifications:
\begin{enumerate}
  \item The first row is the starting state.
  \item The asterisks (``*'') indicate the accepting states.
\end{enumerate}

\begin{table}[H]
  \begin{tabular}{|c|c|c|ll}
    \cline{1-3}
           & 0     & 1     &  & \\ [1ex]   \cline{1-3}
    *$q_0$ & $q_1$ & $q_0$ &  & \\ [0.5ex] \cline{1-3}
    $q_1$  & $q_3$ & $q_2$ &  & \\ [0.5ex] \cline{1-3}
    $q_2$  & $q_3$ & $q_0$ &  & \\ [0.5ex] \cline{1-3}
    *$q_3$ & $q_3$ & $q_3$ &  & \\ [0.5ex] \cline{1-3}
  \end{tabular}
\end{table}

% ******************************
\subsection{Tabular NFAs} \label{automata:tabular-nfas}
% ******************************

Because an NFA is nondeterministic, the state of the NFA after a transition are described by a set of states in the NFA
due to the massive parallelism, so the transitions are actually from one set of states to another set of states.

For example, given the following NFA:

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                {$q_0$};
  \node[state]           (q_1) [right of=q_0] {$q_1$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_3) [right of=q_2] {$q_3$};

  \path[->]
  (q_0) edge [loop above] node {$\Sigma$} ()
  (q_0) edge              node {a}        (q_1)
  (q_1) edge              node {b}        (q_2)
  (q_2) edge              node {a}        (q_3)
  ;
\end{tikzpicture}

We can use the following adjacent matrix to describe it:

\begin{table}[H]
  \begin{tabular}{|c|c|c|ll}
    \cline{1-3}
                         & a                   & b              &  & \\ [1ex]   \cline{1-3}
    \{$q_0$\}            & \{$q_0, q_1$\}      & \{$q_0$\}      &  & \\ [0.5ex] \cline{1-3}
    \{$q_0, q_1$\}       & \{$q_0, q_1$\}      & \{$q_0, q_2$\} &  & \\ [0.5ex] \cline{1-3}
    \{$q_0, q_2$\}       & \{$q_0, q_1, q_3$\} & \{$q_0$\}      &  & \\ [0.5ex] \cline{1-3}
    *\{$q_0, q_1, q_3$\} & \{$q_0, q_1$\}      & \{$q_0, q_2$\} &  & \\ [0.5ex] \cline{1-3}
  \end{tabular}
\end{table}

This can be viewed as a DFA as follows:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q0) {\{$q_0$\}};
  \node[state]           (q0q1) [right of=q0] {\{$q_0, q_1$\}};
  \node[state]           (q0q2) [below of=q0q1]{\{$q_0, q_2$\}};
  \node[state,accepting] (q0q1q3) [right of=q0q2] {\{$q_0, q_1, q_3$\}};

  \path[->]
  (q0) edge [loop above]           node {b} ()
  (q0) edge                        node {a} (q0q1)
  (q0q1) edge [loop above]         node {a} ()
  (q0q1) edge                      node {b} (q0q2)
  (q0q2) edge [bend right, swap]   node {a} (q0q1q3)
  (q0q2) edge                      node {b} (q0)
  (q0q1q3) edge [bend right, swap] node {b} (q0q2)
  (q0q1q3) edge [swap]             node {a} (q0q1)
  ;
\end{tikzpicture}

% =============================================================================
\section{Concatenation}
% =============================================================================

% ******************************
\subsection{String concatenation}
% ******************************

If $w \in \Sigma^*$ and $x \in \Sigma^*$, the \textbf{concatenation} of $w$ and $x$, denoted $wx$, is the string formed
by tacking all the characters of $x$ onto the end of $w$. For example, if if $w = quo$ and $x = kka$, the concatenation
$wx = quokka$.

\begin{itemize}
  \item The empty string $\epsilon$ is the \textbf{identity element} for concatenation: $w\epsilon = \epsilon w = w$.
  \item Concatenation is \textbf{associative}: $wxy = w(xy) = (wx)y$
\end{itemize}

% ******************************
\subsection{Language concatenation}
% ******************************

The \textbf{concatenation} of two languages $L_1$ and $L_2$ over the alphabet $\Sigma$ is the language
\[ L_1L_2 = \{ wx \in \Sigma^* | w \in L_1 \and x \in L_2 \} \]

For example: Let $\Sigma$ = \{ a, b, $\ldots$, z, A, B, $\ldots$, Z \}. Define the following languages over $\Sigma$:
\begin{itemize}
  \item Noun = \{ Puppy, Rainbow, Whale, $\ldots$ \}
  \item Verb = \{ Hugs, Juggles, Loves, $\ldots$ \}
  \item The = \{ The \}
\end{itemize}

The language ``TheNounVerbTheNoun'' is:
\[ \{ ThePuppyHugsTheWhale, TheWhaleLovesTheRainbow, TheRainbowJugglesTheRainbow, \ldots \} \]

% ******************************
\subsection{Language exponentiation}
% ******************************

We can define language ``exponentiation'' as follows:
\begin{enumerate}
  \item The base: $L^0 = \{ \epsilon \}$
  \item The extension: $L^{n+1} = LL^n$ where $n \ge 0$.
\end{enumerate}

\colorbox{red}{TODO(ywen)}: Question to ponder: Why defne $L^0 = \{ \epsilon \}$?

\colorbox{red}{TODO(ywen)}: Question to ponder: What is $\emptyset^0$?

% ******************************
\subsection{The Kleene closure}
% ******************************

The Kleene closure is the language of all possible ways of concatenating zero or more strings in $L$ together, possibly
with repetition defined as \[ L^* = \{ w \in \Sigma^* | \exists n \in \mathbb{N}: w \in L^n \} \]

\colorbox{red}{TODO(ywen)}: Question to ponder: What is $\emptyset^*$?

For example, if $L = \{a, b\}$, then $L^* = \{ \epsilon, a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, bab, bba, bbb,
  \ldots \}$.

% =============================================================================
\section{The Regular Languages}
% =============================================================================

% ******************************
\subsection{Definition}
% ******************************

A language $L$ is called a \textbf{regular language} if there exists a DFA $D$ such that $\mathfrak{L}(D) = L$.
If $L$ is a language and $\mathfrak{L}(D) = L$, we say that $D$ \textbf{recognizes} the language $L$.

\colorbox{lime}{\textbf{NOTE(ywen)}}: The section \ref{automata:automata-and-languages} talks about the languages from
the perspective of a DFA: given a DFA, the strings that this DFA accepts form the language of this DFA. Here, we are
talking about the languages from the perspective of the language itself: given a language, how can we tell if it is
regular or not? It depends on whether there exists a DFA to define this language.

% ******************************
\subsection{NFA's language and DFA's language}
% ******************************

Remember that the language of an automaton is the set of the inputs that the automaton accepts. Therefore, the language
of the NFA in \ref{automata:tabular-nfas} is the same as the language of the DFA which was converted from the NFA.

\textbf{Theorem}: A language $L$ is regular $\Leftrightarrow$ $\exists N: \mathfrak{L}(N) = L$. (See the course slides
for a sketchy proof.)

% ******************************
\subsection{Properties}
% ******************************

Given a language $L \subseteq \Sigma^*$, the \textbf{complement} of that language (denoted $\bar{L}$) is the language of
all strings in $\Sigma^*$ that aren't in $L$. Formally: $L = \Sigma^* - L$.

If $L$ is a regular language, that means there exists a DFA $D$ that recognizes $L$. We can construct the DFA of $L$'s
complement by changing all the accepting states in $D$ to non-accepting states, and changing all the non-accepting
states in $D$ to accepting states. For example:

L = \{ w $\in$ \{a, b\}* $|$ w contains ``aa'' as a substring \}

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial]   (q_0)                {$q_0$};
  \node[state]           (q_1) [right of=q_0] {$q_1$};
  \node[state,accepting] (q_2) [right of=q_1] {$q_2$};

  \path[->]
  (q_0) edge [loop above] node {b} ()
  (q_0) edge [bend left]  node {a}        (q_1)
  (q_1) edge              node {a}        (q_2)
  (q_1) edge [bend left]  node {b}        (q_0)
  (q_2) edge [loop right] node {$\Sigma$} (q_3)
  ;
\end{tikzpicture}

$\bar{L}$ = \{ w $\in$ \{a, b\}* $|$ w \textbf{does not} contain ``aa'' as a substring \}

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \tikzstyle{every state}=[fill={rgb:black,1;white,10}]

  \node[state,initial,accepting] (q_0)                {$q_0$};
  \node[state,accepting]         (q_1) [right of=q_0] {$q_1$};
  \node[state]                   (q_2) [right of=q_1] {$q_2$};

  \path[->]
  (q_0) edge [loop above] node {b} ()
  (q_0) edge [bend left]  node {a}        (q_1)
  (q_1) edge              node {a}        (q_2)
  (q_1) edge [bend left]  node {b}        (q_0)
  (q_2) edge [loop right] node {$\Sigma$} (q_3)
  ;
\end{tikzpicture}

Therefore: If $L$ is a regular language, then $\bar{L}$ is also a regular language. As a result, we say that the
regular languages are \textbf{closed under complementation}.

We also have: If $L_1$ and $L_2$ are regular languages, then:
\begin{enumerate}
  \item $L_1 \cup L_2$ is regular. (The proof uses $\epsilon$-transitions of NFAs.)
  \item $L_1 \cap L_2$ is regular. (The proof uses De Morgan laws.)
  \item $L_1L_2$ is regular.
  \item ${L_1}^*$ and ${L_2}^*$ are regular.
\end{enumerate}

% =============================================================================
%
% Regular Expression
%
% =============================================================================

\chapter{Regular Expression}

% =============================================================================
\section{Regular expressions}
% =============================================================================

% ******************************
\subsection{Definition}
% ******************************

In the chapter of automata, we described a language using \textbf{graphical} DFAs and NFAs. \textbf{Regular expressions}
are a way of describing a language via a \textbf{string representation}. Therefore, a regular expression is equivalent
to a DFA or an NFA.

% ******************************
\subsection{Atomic regular expressions}
% ******************************

The regular expressions begin with three simple building blocks:
\begin{enumerate}
  \item The symbol $\emptyset$ represents the empty language $\emptyset$.
  \item For any $\mathbf{a} \in \Sigma$, the symbol \textbf{a} is a regular expression for the \textbf{language \{a\}}.
  \item The symbol $\epsilon$ represents the \textbf{language \{$\epsilon$\}}. But remember that:
        \begin{itemize}
          \item $\{\epsilon\} \ne \emptyset$
          \item $\{\epsilon\} \ne \epsilon$
        \end{itemize}
\end{enumerate}

\colorbox{lime}{\textbf{NOTE(ywen)}}: Don't confuse a regular expression (which describes a language) and a string
(which is an element in a language which is a set of strings).

% ******************************
\subsection{Compound regular expressions}
% ******************************

If $R_1$ and $R_2$ are two regular expressions:
\begin{enumerate}
  \item $(R_1)$ is a regular expression with the \textbf{same meaning} as $R_1$.
  \item ${R_1}^*$ is a regular expression for the \textbf{Kleene closure} of the language of $R_1$. (The same is
        applicable to $R_2$.)
  \item $R_1R_2$ is a regular expression for the \textbf{concatenation} of the languages of $R_1$ and $R_2$.
  \item $R_1 \cup R_2$ is a regular expression for the \textbf{union} of the languages of $R_1$ and $R_2$.
\end{enumerate}

The operator precedence is \[ (R_1) > {R_1}^* > R_1R_2 > R_1 \cup R_2 \]. So $\mathbf{ab^*c \cup d}$ is parsed as
$\mathbf{((a(b^*))c) \cup d}$.

% ******************************
\subsection{Language of a regular expression}
% ******************************

Because a regular expression is equivalent to a DFA or an NFA, the language of a regular expression is the language
that the regular expression describes.

\begin{itemize}
  \item The atomic ones:
        \begin{enumerate}
          \item $\mathfrak{L}(\emptyset) = \emptyset$
          \item $\mathfrak{L}(\epsilon) = \{ \epsilon \}$
          \item $\mathfrak{L}(a) = \{ a \}$
        \end{enumerate}
  \item The compund ones:
        \begin{enumerate}
          \item $\mathfrak{L}((R)) = \mathfrak{L}(R)$
          \item $\mathfrak{L}(R^*) = \mathfrak{L}(R)^*$
          \item $\mathfrak{L}(R_1R_2) = \mathfrak{L}(R_1)\mathfrak{L}(R_2)$
          \item $\mathfrak{L}(R_1 \cup R_2) = \mathfrak{L}(R_1) \cup \mathfrak{L}(R_2)$
        \end{enumerate}
\end{itemize}

Some theorems:
\begin{itemize}
  \item If $R$ is a regular expression, then $\mathfrak{L}(R)$ is regular.
  \item If $L$ is a regular language, then there is a regular expression for $L$.
\end{itemize}

% ******************************
\subsection{Converting an NFA to a regular expression}
% ******************************

The key idea of converting an NFA to a regular expression is to convert an arbitrary NFA that may contain \textbf{many
  states} into the following general form that consists of only \textbf{two states}:

\begin{tikzpicture}[shorten >=1pt,node distance=4cm,auto]
  \node[state,initial]   (q_0)                {$q_0$};
  \node[state,accepting] (q_1) [right of=q_0] {$q_1$};

  \path[->]
  (q_0) edge node {some-regex} (q_1)
  ;
\end{tikzpicture}

Then ``\textbf{some-regex}'' is the regular expression that is equivalent to this NFA.

For example, given the following NFA, where $R_n$ represents some regular expression:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_1)                {$q_1$};
  \node[state,accepting] (q_2) [right of=q_1] {$q_2$};

  \path[->]
  (q_1) edge [loop above] node {$R_{11}$} ()
  (q_1) edge [bend left]  node {$R_{12}$} (q_2)
  (q_2) edge [loop above] node {$R_{22}$} ()
  (q_2) edge [bend left]  node {$R_{21}$} (q_1)
  ;
\end{tikzpicture}

the major steps of conversion are described as follows:

Step 1: Add a new starting state $q_s$ and a new accepting state $q_f$. Connect $q_s$ and $q_f$ using $\epsilon$-
transitions:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state]           (q_1) [right of=q_s] {$q_1$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge              node {$\epsilon$} (q_1)
  (q_1) edge [loop above] node {$R_{11}$}   ()
  (q_1) edge [bend left]  node {$R_{12}$}   (q_2)
  (q_2) edge [loop above] node {$R_{22}$}   ()
  (q_2) edge [bend left]  node {$R_{21}$}   (q_1)
  (q_2) edge              node {$\epsilon$} (q_f)
  ;
\end{tikzpicture}

Step 2: Add a new transition from $q_s$ to $q_2$ to eliminate the transitions $\epsilon \rightarrow R_{11} \rightarrow
  R_{12}$.

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state]           (q_1) [right of=q_s] {$q_1$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge [red]                node {$\epsilon$}                   (q_1)
  (q_s) edge [bend left=60,blue]  node {$\epsilon {R_{11}}^* R_{12}$} (q_2)
  (q_1) edge [loop above,red]     node {$R_{11}$}                     ()
  (q_1) edge [bend left,red]      node {$R_{12}$}                     (q_2)
  (q_2) edge [loop above]         node {$R_{22}$}                     ()
  (q_2) edge [bend left]          node {$R_{21}$}                     (q_1)
  (q_2) edge                      node {$\epsilon$}                   (q_f)
  ;
\end{tikzpicture}

Step 3: Add a new transition from $q_2$ back to itself to eliminate the transitions $R_{21} \rightarrow R_{11}
  \rightarrow R_{12}$.

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state]           (q_1) [right of=q_s] {$q_1$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge                      node {$\epsilon$}                 (q_1)
  (q_s) edge [bend left=60]       node {${R_{11}}^* R_{12}$}        (q_2)
  (q_1) edge [loop above,red]     node {$R_{11}$}                   ()
  (q_1) edge [bend left,red]      node {$R_{12}$}                   (q_2)
  (q_2) edge [loop above]         node {$R_{22}$}                   ()
  (q_2) edge [bend left,red]      node {$R_{21}$}                   (q_1)
  (q_2) edge                      node {$\epsilon$}                 (q_f)
  (q_2) edge [loop below,blue]    node {$R_{21} {R_{11}}^* R_{12}$} ()
  ;
\end{tikzpicture}

Step 4: Because all the in- and out-transitions of $q_1$ have been equivalently described by other transitions from
$q_s$ to $q_2$, $q_1$ can be eliminated from the NFA now:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge              node {${R_{11}}^* R_{12}$}        (q_2)
  (q_2) edge [loop above] node {$R_{22}$}                   ()
  (q_2) edge              node {$\epsilon$}                 (q_f)
  (q_2) edge [loop below] node {$R_{21} {R_{11}}^* R_{12}$} ()
  ;
\end{tikzpicture}

Step 5: Merge the two self-transitions on $q_2$:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge                   node {${R_{11}}^* R_{12}$}                        (q_2)
  (q_2) edge                   node {$\epsilon$}                                 (q_f)
  (q_2) edge [loop below,blue] node {$R_{22} \cup (R_{21} {R_{11}}^* R_{12})$}   ()
  ;
\end{tikzpicture}

Step 6: Similarly, we can add a new transition from $q_s$ to $q_f$ to eliminate the transitions in between:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state]           (q_2) [right of=q_1] {$q_2$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge [bend left,blue] node {${R_{11}}^* R_{12} (R_{22} \cup (R_{21} {R_{11}}^* R_{12}))^* \epsilon$} (q_f)
  (q_s) edge [red]            node {${R_{11}}^* R_{12}$}                                                     (q_2)
  (q_2) edge [red]            node {$\epsilon$}                                                              (q_f)
  (q_2) edge [loop below,red] node {$R_{22} \cup (R_{21} {R_{11}}^* R_{12})$}                                ()
  ;
\end{tikzpicture}

Step 7: Because all the in- and out-transitions of $q_2$ have been equivalently described by other transitions from
$q_s$ to $q_f$, $q_2$ can be eliminated from the NFA now:

\begin{tikzpicture}[shorten >=1pt,node distance=3cm,auto]
  \node[state,initial]   (q_s)                {$q_s$};
  \node[state,accepting] (q_f) [right of=q_2] {$q_f$};

  \path[->]
  (q_s) edge node {${R_{11}}^* R_{12} (R_{22} \cup (R_{21} {R_{11}}^* R_{12}))^*$} (q_f)
  ;
\end{tikzpicture}

Therefore, the regular expression for the original NFA is
\[ {R_{11}}^* R_{12} (R_{22} \cup (R_{21} {R_{11}}^* R_{12}))^* \]

% =============================================================================
\section{DFAs, NFAs, and Regexp}
% =============================================================================

\begin{tikzpicture}[shorten >=1pt,node distance=6cm,auto]
  \node[state]   (q_0)                {$DFA$};
  \node[state]   (q_1) [right of=q_0] {$NFA$};
  \node[state]   (q_2) [right of=q_1] {$Regexp$};

  \path[->]
  (q_0) edge [bend left] node {direct conversion}    (q_1)
  (q_1) edge [bend left] node {state elimination}    (q_2)
  (q_2) edge [bend left] node {Thompson's algorithm} (q_1)
  (q_1) edge [bend left] node {subset construction}  (q_0)
  ;
\end{tikzpicture}

% =============================================================================
%
% Nonregular Languages
%
% =============================================================================

\chapter{Nonregular Languages}

\begin{itemize}
  \item Regular languages correspond to problems that \textbf{can be solved} with \textbf{finite} memory.
  \item Nonregular languages correspond to problems that \textbf{cannot be solved} with \textbf{finite} memory.
\end{itemize}

% =============================================================================
\section{Example 1} \label{automata:nonregexp:example1}
% =============================================================================

Try to design an NFA for the language $E = \{ a^nb^n | n \in \mathbb{N} \}$.

Think: Can $a^2$ and $a^4$ be represented by the same state?

\begin{tikzpicture}[shorten >=1pt,node distance=2cm,auto]
  \node[state,initial]                   (q_0)                       {};
  \node[state]                           (q_1) [right of=q_0]        {$\ldots$};
  \node[state with output,fill=yellow]   (q_a2a4) [right of=q_1]     {$a^2$ \nodepart{lower} $a^4$};
  \node[state]                           (q_2) [right of=q_a2a4]     {$\ldots$};
  \node[state with output,fill=yellow]   (q_a2b4a4b4) [right of=q_2] {$a^2b^4$ \nodepart{lower} $a^4b^4$};

  \path[->]
  (q_0) edge node {} (q_1)
  (q_1) edge node {} (q_a2a4)
  (q_a2a4) edge node {} (q_2)
  (q_2) edge node {} (q_a2b4a4b4)
  ;
\end{tikzpicture}

They can't be represented by the same state (i.e., this state represents both $a^2$ and $a^4$ at the same time),
because if they could, then appending $b^4$ would lead the NFA to a state that represents the following two \textbf{at
  the same time}:
\begin{itemize}
  \item $a^2b^4$
  \item $a^4b^4$
\end{itemize}

Now we end up in a dilemma:
\begin{itemize}
  \item If we make it an accepting state in order to accept $a^4b^4 \in E$, then we also accept $a^2b^4 \notin E$.
  \item If we make it a rejecting state in order to reject $a^2b^4 \notin E$, then we also reject $a^4b^4 \in E$.
\end{itemize}

% =============================================================================
\section{Distinguishability}
% =============================================================================

Let $L$ be an arbitrary language over $\Sigma$. Two strings $x \in \Sigma^*$ and $y \in \Sigma^*$ are called
\textbf{distinguishable} relative to $L$ if there is a string $w \in \Sigma^*$ such that either $xw$ or $yw$ is in $L$.
We denote this by writing $x \not\equiv_L y$.

Formally, we say $x \not\equiv_L y$ if the following is true (described in first-order logic):

\[ \exists w \in \Sigma^*. (xw \in L \leftrightarrow yw \notin L) \]

\textbf{Theorem}: Let $L$ be an arbitrary language over $\Sigma$. Let $x \in \Sigma^*$ and $y \in \Sigma^*$ be strings
where $x \not\equiv_L y$. Then if $D$ is \textbf{any} DFA for $L$, then $D$ must end in different states when run on
inputs $x$ and $y$.

% ******************************
\subsection{Proof: Example 1 is not regular}
% ******************************

\textbf{Lemma}: In the language $E$ in \ref{automata:nonregexp:example1}, if $m, n \in \mathbb{N}$ and $m \neq n$, then
$a^m \not\equiv_E a^n$.

\textbf{Proof}: Let $a^m$ and $a^n$ be strings where $m \neq n$. Then $a^mb^m \in E$ and $a^nb^m \notin E$. According
to the definition of ``distinguishable'', we see that $a^m \not\equiv_E a^n$, as required. $\blacksquare$

\textbf{Theorem}: The language $E$ in \ref{automata:nonregexp:example1} is not regular.

\textbf{Proof}: Suppose for the sake of contradiction that $E$ is regular. Let $D$ be a DFA for $E$, and let $k$ be the
number of states in $D$. Consider the strings $a^0$, $a^1$, $a^2$, $\ldots$, $a^k$. This is a collection of $k+1$
strings and there are only $k$ states in $D$. Therefore, by the pigeonhole principle, there must be two distinct
strings $a^m$ and $a^n$ that end in the same state when run through $D$. Our lemma tells us that $a^m \not\equiv_E a^n$,
so by our earlier theorem we know that $a^m$ and $a^n$ cannot end in the same state when run through $D$. We have
reached a contradiction, so our assumption must have been wrong. Therefore, $E$ is not regular. $\blacksquare$

% =============================================================================
\section{Distinguishing Sets}
% =============================================================================

Let $L$ be a language over $\Sigma$. A \textbf{distinguishing set} for $L$ is a set $S \subset \Sigma^*$ where the
following is true:

\[ \forall x \in S. \forall y \in S. (x \neq y \rightarrow x \not\equiv_L y) \]

Intuitively, it means if you pick any two strings that are not equal to one another, then they are distinguishable
relative to $L$.

% =============================================================================
\section{Myhill-Nerode theorem}
% =============================================================================

\textbf{Theorem} If $L$ is a language and $S$ is a distinguishing set for $L$ that contains \textbf{infinitely many}
strings, then $L$ is not regular.

\textbf{Proof}: Let $L$ be an arbitrary language over $\Sigma$ and let $S$ be a distinguishing set for L that contains
infinitely many strings. We will show that $L$ is not regular.

Suppose for the sake of contradiction that $L$ is regular. This means that there must be some DFA $D$ for $L$. Let $k$
be the number of states in $D$. Since there are infinitely many strings in $S$, we can choose $k+1$ distinct strings
from $S$ and consider what happens when we run $D$ on all of those strings. Because there are only $k$ states in $D$
and we've chosen $k+1$ strings from $S$, by the pigeonhole principle we know that at least two strings from $S$ must
end in the same state in $D$. Choose any two such strings and call them $x$ and $y$.

Because $x \neq y$ and $S$ is a distinguishing set for $L$, we know that $x \not\equiv_L y$. Our earlier theorem
therefore tells us that when we run $D$ on inputs $x$ and $y$, they must end up in different states. But this is
impossible - we chose $x$ and $y$ precisely because they end in the same state when run through $D$.

We have reached a contradiction, so our assumption must have been wrong. Thus $L$ is not a regular language.
$\blacksquare$

% ******************************
\subsection{Using the Myhill-Nerode theorem}
% ******************************

To use the Myhill-Nerode theorem, we only need to find a set $S$ in $L$ that is infinite and is a distinguishing set.
For example:
\begin{itemize}
  \item For the language $E = \{ a^nb^n | n \in \mathbb{N} \}$, $S = \{ a^n | n \in \mathbb{N} \}$.
  \item For the language $EQ = \{ w\stackrel{?}{=}w | w \in \{a, b\}^* \}$, $S = \{a, b\}^*$
\end{itemize}

% =============================================================================
%
% Context-Free Languages
%
% =============================================================================

\chapter{Context-Free Languages}

% =============================================================================
\section{Context-free grammar (CFG) and context-free languages (CFL)}
% =============================================================================

% ******************************
\subsection{Context-free grammar (CFG)}
% ******************************

A \textbf{context-free grammar} (\textbf{CFG}) is a \textbf{recursive} set of rules that define a language. It consists
of the following items:
\begin{enumerate}
  \item A set of \textbf{nonterminal symbols} (i.e., the \textbf{variables}).
  \item A set of \textbf{terminal symbols} (i.e., the \textbf{alphabet} of the CFG).
  \item A set of \textbf{production rules} about how each nonterminal can be replaced by a string of terminals and
        nonterminals.
  \item A \textbf{start symbol} (which must be a nonterminal) that begins the derivation. By convention, the start
        symbol is the one on the left-hand side of the first production.
\end{enumerate}

A sequence of zero or more steps where nonterminals are replaced by the right-hand side of a production rule is called
a \textbf{derivation}, denoted $\alpha \Rightarrow^* \omega$.

% ******************************
\subsection{CFG Example}
% ******************************

Long form:

\begin{itemize}
  \item \textcolor{red}{\textbf{Expr}} $\rightarrow$ \textcolor{blue}{int}
  \item \textcolor{red}{\textbf{Expr}} $\rightarrow$ \textcolor{red}{\textbf{Expr Op Expr}}
  \item \textcolor{red}{\textbf{Expr}} $\rightarrow$ \textcolor{blue}{(}\textcolor{red}{\textbf{Expr}}
        \textcolor{blue}{)}
  \item \textcolor{red}{\textbf{Op}} $\rightarrow$ \textcolor{blue}{$+$}
  \item \textcolor{red}{\textbf{Op}} $\rightarrow$ \textcolor{blue}{$-$}
  \item \textcolor{red}{\textbf{Op}} $\rightarrow$ \textcolor{blue}{$\times$}
  \item \textcolor{red}{\textbf{Op}} $\rightarrow$ \textcolor{blue}{$\div$}
\end{itemize}

Short form:

\begin{itemize}
  \item \textcolor{red}{\textbf{Expr}} $\rightarrow$ \textcolor{blue}{int} $|$ \textcolor{red}{\textbf{Expr Op Expr}}
        $|$ \textcolor{blue}{(}\textcolor{red}{\textbf{Expr}}\textcolor{blue}{)}
  \item \textcolor{red}{\textbf{Op}} $\rightarrow$ \textcolor{blue}{$+$} $|$ \textcolor{blue}{$-$} $|$
        \textcolor{blue}{$\times$} $|$ \textcolor{blue}{$\div$}
\end{itemize}

A derivation: \textcolor{red}{\textbf{Expr}} $\Rightarrow^*$ \textcolor{blue}{int + int}.

% ******************************
\subsection{Context-free languages (CFL)}
% ******************************

A language $L$ is called a \textbf{context-free language} (\textbf{CFL}) if there is a CFG $G$ such that $L =
  \mathfrak{L}(G)$.

% ******************************
\subsection{Context-free languages (CFL) and regular languages}
% ******************************

Every regular language is a context-free language; but not all context-free languages are regular languages.

\begin{tikzpicture}
  \node[
    rectangle,
    draw=blue,
    text=red,
    fill = green!30!yellow,
    minimum width = 10cm,
    minimum height = 6cm
  ] (r) at (0.8,0) {};
  \node[left, black] at (5, -2.5) {All languages};

  % Set CFLs, the enclosing set.
  \node [ellipse,
    draw=blue!60,
    fill=cyan!40,
    line width=1mm,
    minimum width=8cm,
    minimum height=5cm,
  ] at (0,0) {};
  \node[left, black, fill=cyan!40] at (3.5,0) {CFLs};

  % Set regular languages, the enclosed set.
  \node [ellipse,
    draw=orange!80,
    fill=yellow!20,
    line width=1mm,
    minimum width=4cm,
    minimum height=2cm] at (-1,0) {Regular languages};
\end{tikzpicture}

For example, the CFG ``\textcolor{red}{\textbf{S}} $\rightarrow$ \textcolor{blue}{a}\textcolor{red}{\textbf{S}}
\textcolor{blue}{b} $|$ \textcolor{blue}{$\epsilon$}'' generates the non-regular language $\{ a^nb^n | n \in \mathbb{N}
  \}$.

% ******************************
\subsection{The extra power}
% ******************************

CFGs derivations essentially have \textbf{unbounded} states. Therefore, they can describe both regular languages (which
only need finite number of states) and non-regular languages (which need infinite number of states).

% =============================================================================
\section{Designing CFGs}
% =============================================================================

See the course slides.

% =============================================================================
%
% Turing Machines
%
% =============================================================================

\chapter{Turing Machines}

% =============================================================================
\section{A Turing machine}
% =============================================================================

% ******************************
\subsection{Definition}
% ******************************

About a Turing machine:
\begin{itemize}
  \item A Turing machine has an \textbf{infinite} tape that consists of \textbf{tape cells}.
  \item At any time, a Turing machine can only see the \textbf{one} tape cell that is pointed at by the \textbf{tape
          head}.
  \item The tape begins with the input string surrounded by infinitely many \textbf{blank cells}. (Therefore, the input
        string cannot contain blank cells.)
  \item A Turing machine is a series of \textbf{instructions} that control a tape head as it moves across the tape.
        \begin{itemize}
          \item The instructions can have \textbf{labels} that mark the starting of different sections of instructions.
                Labels have no effect when executed. We just move to the next instruction.
          \item The special label ``Start'' means ``begin here''.
          \item \textbf{If} \textcolor{blue}{symbol} \textcolor{red}{command}: If the character under the tape head is
                \textcolor{blue}{symbol}, then execute \textcolor{red}{command}; otherwise nothing happens.
          \item \textbf{If Not} \textcolor{blue}{symbol} \textcolor{red}{command}: If the character under the tape head
                is \textbf{not} \textcolor{blue}{symbol}, then execute \textcolor{red}{command}; otherwise nothing
                happens.
          \item \textbf{Write} \textcolor{blue}{symbol$|$Blank}: Write \textcolor{blue}{symbol$|$Blank} to the cell
                under the tape head.
          \item \textbf{Move} \textcolor{blue}{Left$|$Right}: Move the tape head to \textcolor{blue}{Left} or
                \textcolor{blue}{Right}.
          \item \textbf{Goto} \textcolor{blue}{label}: Jump to the \textcolor{blue}{label}.
          \item \textbf{Return} \textcolor{blue}{True$|$False}: The Turing machine stops at \textbf{Return}. The result
                \textcolor{blue}{True} means the Turing machine accepts the input; the result \textcolor{blue}{False}
                means the Turing machine rejects the input.
        \end{itemize}
\end{itemize}

\colorbox{lime}{\textbf{NOTE(ywen)}}: Remember that a Turing machine is not just the tape. The tape is the information
storage that the Turing machine can operate on. The part that distinguishes one Turing machine from another Turing
machine is the list of the instructions. Different lists of instructions define different Turing machines.

% ******************************
\subsection{Powers of Turing machines}
% ******************************

Turing machines are equal in power to \textbf{idealized computers}. That is, any computation that can be done on a
Turing machine can be done on an idealized computer and vice-versa.

Because the list of instructions distinguishes one Turing machine from another, we can see that:
\begin{itemize}
  \item A Turing machine can implement conditional branching (by using \textbf{If}).
  \item A Turing machine can implement loops (by using \textbf{Goto}, \textbf{If}, and labels).
  \item A Turing machine can do arithmetics such as addition, subtraction, powers of numbers, ceiling, floor, etc.
        \colorbox{red}{\textcolor{yellow}{TODO(ywen)}}: Figure out how to implement them using Turing machines.
  \item A Turing machine can maintain variables.
  \item A Turing machine can implement ``helper'' functions that work together to solve larger problems.
  \item A Turing machine can maintain strings and arrays (by storing them with special separator character).
  \item A Turing machine can support pointers.
  \item A Turing machine can support function call and return.
\end{itemize}

% ******************************
\subsection{Designing Turing machines}
% ******************************

See the course slides for examples.

% ******************************
\subsection{Effective computation}
% ******************************

An \textbf{effective method of computation} is a form of computation with the following properties:
\begin{itemize}
  \item The computation consists of a set of steps.
  \item There are fixed rules governing how one step leads to the next.
  \item Any computation that yields an answer does so in finitely many steps.
  \item Any computation that yields an answer always yields the correct answer.
\end{itemize}

\textbf{Church-Turing Thesis}: Every effective method of computation is either \textbf{equivalent to or weaker than} a
Turing machine.

% =============================================================================
\section{Decidability and Recognizability}
% =============================================================================

Let $M$ be a Turing machine:
\begin{itemize}
  \item $M$ \textbf{accepts} a string $w$ if it returns $true$ on $w$.
  \item $M$ \textbf{rejects} a string $w$ if it returns $false$ on $w$.
  \item $M$ \textbf{loops infinitely} (or just \textbf{loops}) on a string $w$ if when run on $w$ it neither returns
        $true$ nor returns $false$.
  \item $M$ \textbf{does not accept} $w$ if it either rejects $w$ or loops on $w$.
  \item $M$ \textbf{does not reject} $w$ if it either accepts $w$ or loops on $w$.
  \item $M$ \textbf{halts} on $w$ if it accepts $w$ or rejects $w$.
\end{itemize}

% ******************************
\subsection{Recognizers}
% ******************************

A Turing machine $M$ is called a \textbf{recognizer} for a language $L$ over $\Sigma$ if the following statement is
true: \[ \forall w \in \Sigma^*. (w \in L \leftrightarrow M \ accepts \ w) \]

\begin{itemize}
  \item If you are absolutely certain that $w \in L$, then running a recognizer for $L$ on $w$ will (eventually)
        confirm this, i.e., eventually, $M$ will accept $w$.
  \item If you don't know whether $w \in L$, running $M$ on $w$ may never tell you anything. $M$ might loop on $w$, but
        you can't differentiate between ``it'll never give an answer'' and ``just wait a bit more.''
\end{itemize}

% ******************************
\subsection{Deciders}
% ******************************

A Turing machine $M$ is called a \textbf{decider} for a language $L$ over $\Sigma$ if the following statements are
true:
\begin{itemize}
  \item $\forall w \in \Sigma^*$. $M$ halts on $w$.
  \item $\forall w \in \Sigma^*$. ($w \in L$ $\leftrightarrow$ $M$ accepts $w$)
  \item $\forall w \in \Sigma^*$. ($w \notin L$ $\leftrightarrow$ $M$ rejects $w$)
\end{itemize}

A \textbf{decision problem} is a type of problem where the goal is to provide a ``yes'' or ``no'' answer.
\textbf{Solving a problem} means ``deciding yes or no''.

% ******************************
\subsection{RE (recognizable languages) and R (decidable languages)}
% ******************************

The class \textbf{RE} consists of all recognizable languages:
\begin{itemize}
  \item RE = \{ $L$ $|$ $L$ is a language and there's a recognizer for $L$ \}.
  \item RE is all the problems with ``yes/no'' answers where ``yes'' answers can be confirmed by a computer.
\end{itemize}

The class \textbf{R} consists of all decidable languages:
\begin{itemize}
  \item R = \{ $L$ $|$ $L$ is a language and there's a decider for $L$ \}.
  \item R is all problems with ``yes/no'' answers that can be fully solved by computers.
\end{itemize}

% =============================================================================
\section{Solving a problem using Turing machines}
% =============================================================================

The model looks like the following illustration:

\begin{tikzpicture}
  \node[left, black, fill=white] at (0,0) {input};
  \draw [-stealth](0,0) -- (1.4,0);

  \node[
    rectangle,
    draw,
    text=red,
    fill = green!30!yellow,
    minimum width = 3cm,
    minimum height = 2cm
  ] (M) at (3,0) {Turing machine};

  \node[draw,
    circle,
    minimum size=1cm,
    fill=green!30,
    label=(accept),
  ] (T) at (7,1.5) {True};

  \node[draw,
    circle,
    minimum size=1cm,
    fill=red!30,
    label=(reject),
  ] (F) at (7,-1.5) {False};

  \draw [-stealth](4.8,0) -- (6.2,1.3);
  \draw [-stealth](4.8,0) -- (6.2,-1.3);
\end{tikzpicture}

A Turing machine's input is a string on some alphabet $\Sigma$. In order to solve a problem using a Turing machine, we
need to translate an arbitrary input object into a string.

% ******************************
\subsection{Encoding of objects}
% ******************************

If ``Obj'' is an object that is \textbf{discrete} and \textbf{finite}, the notation $\langle Obj \rangle$ means some
way of encoding ``Obj'' as a string.

Given a list of $Obj_1, Obj_2, \ldots, Obj_n$, $\langle Obj_1, Obj_2, \ldots, Obj_n \rangle$ is a \textbf{single string}
that denotes the encoding of this \textbf{entire list} of objects.

% ******************************
\subsection{The universal Turing machine}
% ******************************

\textbf{Theorem (Turing, 1936)}: There is a Turing machine $U_{TM}$ called the \textbf{universal Turing machine} that,
when run on an input of the form $\langle M, w\rangle$, where $M$ is a Turing machine and $w$ is a string, simulates
$M$ running on $w$ and does whatever $M$ does on $w$ (accepts, rejects, or loops).

In other words:
\begin{itemize}
  \item If $M$ accepts $w$, then $U_{TM}$ accepts $\langle M, w\rangle$.
  \item If $M$ rejects $w$, then $U_{TM}$ rejects $\langle M, w\rangle$.
  \item If $M$ loops on $w$, then $U_{TM}$ loops on $\langle M, w\rangle$.
\end{itemize}

Here we encode the Turing machine $M$ and its input $w$ into a string $\langle M, w\rangle$ (over some alphabet
$\Sigma$) and feed it into $U_{TM}$. So all the possible strings in the form of $\langle M, w\rangle$ that are accepted
by $U_{TM}$ form the language that's recognized by $U_{TM}$. In contrast, because there exists some $w$ that's not
accepted by $M$, not all the strings in the form of $\langle M, w\rangle$ belong to the language that $U_{TM}$
recognizes.

Let $A_{TM}$ be the language that's recognized by $U_{TM}$. That means $\forall x \in \Sigma^*.$ ($U_{TM}$ accepts $x$
$\leftrightarrow$ $x \in A_{TM}$), or, equivalently, $\forall M. \forall w \in \Sigma^*.$ ($U_{TM}$ accepts
$\langle M, w\rangle$ $\leftrightarrow$ $\langle M, w\rangle$ $\in$ $A_{TM}$ )

So we have $A_{TM}$ = \{ $\langle M, w\rangle$ $|$ $M$ is a Turing machine and $M$ accepts $w$ \}

% =============================================================================
\section{$A_{TM}$ is recognizable but not decidable}
% =============================================================================

\textbf{Theorem}: The language $A_{TM}$ is recognizable but not decidable. In other words, $A_{TM} \notin \textbf{R}$.

Intuitively, because $U_{TM}$ has the same behavior as the given Turing machine $M$, $U_{TM}$ doesn't halt when $M$
doesn't halt. We already know the $M$ for the hailstone sequence is only recognizable, so $U_{TM}$ can't be decidable.

But to prove this, we need more knowledge.

% ******************************
\subsection{Self-defeating objects and the fortune teller}
% ******************************

A \textbf{self-defeating object} is an object whose essential properties ensure it \textbf{does not exist}.

For example: ``the largest integer'' is a self-defeating object. Suppose we use ``N'' to denote the largest integer,
then ``N+1'' is even larger than ``N'' so ``N'' doesn't exist. Here we use the object ``N'' itself to construct
something that undermines ``N'', hence ``self-defeating''.

A fortune teller who charges \$100 and tells about the future is a self-defeating object too. The
trickster has the following request to the fortune teller:
\begin{enumerate}
  \item I have a ``yes/no'' question about the future.
  \item If your answer is ``yes'', I'll pay \$1,000.
  \item If your answer is ``no'', I'll pay \$500.
\end{enumerate}

Suppose the fortune teller accepts the request because he wants to make more money. The trickster's question is:
``Am I going to pay \$500?''
\begin{itemize}
  \item If the fortune teller says ``yes'', according to the previous request, the trickster will pay \$1,000, but
        according to the question itself, the trickster will pay \$500.
  \item If the fortune teller says ``no'', according to the previous request, the trickster will pay \$500, but
        according to the question itself, the trickster will not pay \$500.
\end{itemize}

The fortune teller can't answer all the questions about the future.

% ******************************
\subsection{Self-Referential programs}
% ******************************

A Turing machine can take itself as the input. Turing machines can take their own code as input, and ask questions
about (or even execute) their own code. In fact, any computing system that's equal in power to a Turing machine possesses some mechanism for self-reference.

A \textbf{quine} is a special kind of self-referential program that, when run, prints its own source code.

% ******************************
\subsection{A self-defeating decider program}
% ******************************

Suppose we have the following decider program:

\begin{lstlisting}[language=C++]
  bool willAccept(string function, string input);
\end{lstlisting}

which
\begin{itemize}
  \item returns $true$ if $function$ returns $true$;
  \item returns $false$ if $function$ does not return $true$.
\end{itemize}

We can construct a $trickster$ function as follows:

\begin{lstlisting}[language=C++]
  bool trickster(string input) {
    // This is a self-referential program.
    string self_ref = /* source code of trickster itself */;
    return !willAccept(self_ref, input);
  }
\end{lstlisting}

As a result, the $willAccept$ is a self-defeating program:
\begin{itemize}
  \item If $willAccept$ says $trickster$ returns $true$, then $trickster$ will return $false$.
  \item If $willAccept$ says $trickster$ returns $false$, then $trickster$ will return $true$.
\end{itemize}

% ******************************
\subsection{Prove $A_{TM} \notin \textbf{R}$}
% ******************************

\textbf{Theorem}: $A_{TM} \notin \textbf{R}$

\textbf{Proof}: By contradiction, assume that $A_{TM} \in \mathbf{R}$. Then there is a decider $D$ for $A_{TM}$. We can
represent $D$ as the following function:

\begin{lstlisting}[language=C++]
  bool willAccept(string function, string w);
\end{lstlisting}

which takes in the source code of a function $function$ and a string $w$, then returns $true$ if $function(w)$ returns
$true$ and returns $false$ otherwise.

Given the decider $willAccept$, consider the following function $trickster$:

\begin{lstlisting}[language=C++]
  bool trickster(string input) {
    string me = /* source code of trickster itself */;
    return !willAccept(me, input);
  }
\end{lstlisting}

Choose a string $w$. We consider two cases:
\begin{enumerate}
  \item $willAccept(me, input)$ returns $true$. Since $willAccept$ decides $A_{TM}$, this means $trickster(w)$ returns
        $true$. However, given how $trickster$ is written, in this case $trickster(w)$ returns $false$.
  \item $willAccept(me, input)$ returns $false$. Since $willAccept$ decides $A_{TM}$, this means $trickster(w)$ doesn't
        return $true$. However, given how $trickster$ is written, in this case $trickster(w)$ returns $true$.
\end{enumerate}

In both cases we reach a contradiction, so our assumption must have been wrong. Therefore, $A_{TM} \notin \textbf{R}$.
$\blacksquare$

% =============================================================================
\section{Relationships of languages}
% =============================================================================

\begin{tikzpicture}
  \node[
    rectangle,
    draw=blue,
    text=red,
    fill = green!30!yellow,
    minimum width = 15cm,
    minimum height = 9cm
  ] (r) at (3,0) {};
  \node[left, black] at (9.5, -4) {All languages};

  % Set RE, the recognizable languages.
  \node [ellipse,
    draw=blue!60,
    fill=red!40,
    line width=1mm,
    minimum width=12cm,
    minimum height=8cm,
  ] at (2,0) {};
  \node[left, black, fill=red!40] at (7.5,0) {RE};

  % Set R, the decidable languages.
  \node [ellipse,
    draw=blue!60,
    fill=orange!40,
    line width=1mm,
    minimum width=10cm,
    minimum height=6cm,
  ] at (1,0) {};
  \node[left, black, fill=orange!40] at (5.5,0) {R};

  % Set CFLs.
  \node [ellipse,
    draw=blue!60,
    fill=cyan!40,
    line width=1mm,
    minimum width=8cm,
    minimum height=4cm,
  ] at (0,0) {};
  \node[left, black, fill=cyan!40] at (3.5,0) {CFLs};

  % Set regular languages.
  \node [ellipse,
    draw=orange!80,
    fill=yellow!20,
    line width=1mm,
    minimum width=4cm,
    minimum height=2cm] at (-1,0) {Regular languages};
\end{tikzpicture}

% =============================================================================
%
% Unsolvable Problems
%
% =============================================================================

\chapter{Unsolvable Problems}

% =============================================================================
\section{The halting problem}
% =============================================================================

The \textbf{halting problem} is: Given a Turing machine $M$ and a string $w$, will $M$ halt when run on $w$?

The language: $HALT$ = \{ $\langle M, w\rangle$ $|$ $M$ is a Turing machine that halts on $w$ \}.

\textbf{Theorem}: $HALT$ is recognizable but not decidable. In other words, $HALT \in \mathbf{RE}$ and $HALT \notin
  \mathbf{R}$.

See the course slides for the proof.

% =============================================================================
\section{The languages in RE}
% =============================================================================

A language $L$ is in \textbf{RE} if, for any string $w$, if you are convinced that $w \in L$, there is some way you
could prove that to someone else.
\begin{itemize}
  \item If you can prove that, that means $w \in L$.
  \item If you can't prove that, that does not mean $w \notin L$. You just don't know whether $w \in L$.
\end{itemize}

% =============================================================================
\section{Verifiers}
% =============================================================================

A \textbf{verifier} for a language $L$ is a Turing machine $V$ with the following two properties:
\begin{itemize}
  \item $V$ halts on all inputs.
  \item $\forall w \in \Sigma^*.$ ($w \in L$ $\leftrightarrow$ ($\exists c \in \Sigma^*.$ $V$ accepts $\langle w,
          c\rangle$)).
\end{itemize}

Some notes about verifiers:
\begin{itemize}
  \item A decider solves a problem; a verifier checks an answer.
  \item If $V$ accepts $\langle w, c \rangle$, we're guaranteed $w \in L$.
  \item If $V$ rejects $\langle w, c \rangle$, then either of the following two:
        \begin{itemize}
          \item $w \in L$, but you gave the wrong $c$.
          \item $w \notin L$, so no possible $c$ will work.
        \end{itemize}
  \item $V$ is neither a recognizer or a decider for $L$.
  \item The job of $V$ is just to check certificates, not to decide membership in $L$. \colorbox{red}{TODO(ywen)}: Why
        is the job of $V$ to check the \textbf{certificates} but not the input string $w$?
  \item In practice, $c$ will likely just be ``some other auxiliary data that helps you out.''
\end{itemize}

% ******************************
\subsection{The verifier for $A_{TM}$}
% ******************************

\begin{lstlisting}[language=C++]
  bool checkWillAccept(string M, string w, int c) {
    // Set up a simulation of M that runs on w;
    for (int i = 0; i < c; i++) {
      // Simulate the next step of M that runs on w;
    }
    return whether M is in an accepting state;
  }
\end{lstlisting}

% =============================================================================
\section{Verifiable languages}
% =============================================================================

\textbf{Theorem}: If $L$ is a language, then there is a verifier for $L$ \textbf{if and only if} $L \in \mathbf{RE}$.

Verifiers are explicitly built to check proofs that strings are in the language.

You can think of a recognizer as a device that ``searches'' for a proof that $w \in L$.

% =============================================================================
\section{Finding a non-RE language}
% =============================================================================

\colorbox{lime}{\textbf{NOTE(ywen)}}: The course slides 151, 152 and 153 show the language of all the Turing machines
that do not accept their encoded strings, or: \{ $\langle M \rangle$ $|$ $M$ is a Turing machine that does not accept
$\langle M \rangle$ \}.

The existence of non-RE languages means \textbf{there are statements that are true but not provable} (i.e., the Godel's
incompleteness theorem).

% =============================================================================
%
% Complexity Theory and P vs NP
%
% =============================================================================

\chapter{Complexity Theory and P vs NP}

% =============================================================================
\section{\textbf{P} and \textbf{NP}}
% =============================================================================

An algorithm runs in \textbf{polynomial time} if its runtime is some polynomial in $n$, i.e., time $O(n^k)$ for some
constant $k \in \mathbb{N}$.

\textbf{Cobham-Edmonds Thesis}: A language $L$ can be \textbf{decided efficiently} if it can be decided in time $O(n^k)$
for some $k \in \mathbb{N}$.

The class \textbf{P} represents problems that can be solved \textbf{efficiently} by a computer.

The class \textbf{NP} represents problems where ``yes'' answers can be verified \textbf{efficiently} by a computer.

% =============================================================================
\section{Reducibility}
% =============================================================================

If problem A's input can be \textbf{polynomial-timely translated} into the input for problem B, and problem B can be
\textbf{polynomial-timely solved}, then we say A is \textbf{polynomial-time reducible} to B, denoted $A \le_p B$.

Intuitively, that means:
\begin{itemize}
  \item A is at most as hard as B.
  \item A can't be harder than B.
  \item B could be harder than A.
\end{itemize}

If $A \le_p B$ and $B \in \mathbf{P}$, then $A \in \mathbf{P}$. If $A \le_p B$ and $B \in \mathbf{NP}$, then
$A \in \mathbf{NP}$.

% =============================================================================
\section{NP-hard and NP-complete}
% =============================================================================

For languages $A$ and $B$, we say $A \le_p B$ if $A$ reduces to $B$ in polynomial time. (Intuitively, $B$ is at least
as hard as $A$. $B$ could be harder than $A$. $A$ can't be harder than $B$.)

We say that a language $L$ is \textbf{NP-hard} if $\forall A \in \mathbf{NP}. A \le_p L$. (Intuitively, $L$ is at least
as hard as any language in \textbf{NP}.)

We say that a language $L$ is $NP-complete$ if $L \in \mathbf{NP}$ and $L$ is \textbf{NP-hard}.

% =============================================================================
%
% References
%
% =============================================================================

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{itemize}
  \item $[1]$ \href{https://web.stanford.edu/class/cs103/}{CS103: Mathematical Foundations of Computing}
\end{itemize}

\end{document}
